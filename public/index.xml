<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dina Arch</title>
    <link>https://dinaarch.netlify.app/</link>
      <atom:link href="https://dinaarch.netlify.app/index.xml" rel="self" type="application/rss+xml" />
    <description>Dina Arch</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dinaarch.netlify.app/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Dina Arch</title>
      <link>https://dinaarch.netlify.app/</link>
    </image>
    
    <item>
      <title>Python basics</title>
      <link>https://dinaarch.netlify.app/courses/example/python/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/courses/example/python/</guid>
      <description>&lt;p&gt;Build a foundation in Python.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/rfscVS0vtbw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the difference between lists and tuples?&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Lists&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lists are mutable - they can be changed&lt;/li&gt;
&lt;li&gt;Slower than tuples&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_list = [1, 2.0, &#39;Hello world&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tuples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuples are immutable - they can&amp;rsquo;t be changed&lt;/li&gt;
&lt;li&gt;Tuples are faster than lists&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_tuple = (1, 2.0, &#39;Hello world&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Is Python case-sensitive?&lt;/summary&gt;
  &lt;p&gt;Yes&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Visualization</title>
      <link>https://dinaarch.netlify.app/courses/example/visualization/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/courses/example/visualization/</guid>
      <description>&lt;p&gt;Learn how to visualize data with Plotly.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hSPmj7mK6ng&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;When is a heatmap useful?&lt;/summary&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Write Plotly code to render a bar chart&lt;/summary&gt;
  &lt;p&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import plotly.express as px
data_canada = px.data.gapminder().query(&amp;quot;country == &#39;Canada&#39;&amp;quot;)
fig = px.bar(data_canada, x=&#39;year&#39;, y=&#39;pop&#39;)
fig.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Statistics</title>
      <link>https://dinaarch.netlify.app/courses/example/stats/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/courses/example/stats/</guid>
      <description>&lt;p&gt;Introduction to statistics for data science.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;The general form of the &lt;strong&gt;normal&lt;/strong&gt; probability density function is:&lt;/p&gt;
&lt;p&gt;$$
f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The parameter $\mu$ is the mean or expectation of the distribution.
$\sigma$ is its standard deviation.
The variance of the distribution is $\sigma^{2}$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the parameter $\mu$?&lt;/summary&gt;
  &lt;p&gt;The parameter $\mu$ is the mean or expectation of the distribution.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://dinaarch.netlify.app/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mediation Lab</title>
      <link>https://dinaarch.netlify.app/post/mediation-lab/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/post/mediation-lab/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(haven) #read_sav()
library(mediation) # mediate() (Tingley, Yamamoto, Hirose, Keele, &amp;amp; Imai, 2014)
library(gvlma) # gvlma()
library(kableExtra) #kable()
library(corrr) #correlate()
library(psych) #mediate()
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;mediation-in-r&#34;&gt;Mediation in R&lt;/h1&gt;
&lt;p&gt;This exampes comes from a tutorial paper on mediation found &lt;a href=&#34;https://www.tqmp.org/RegularArticles/vol13-3/p148/p148.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
This is an open source paper that shared the data set used in this example.
The following is code and interpretation are based on this paper.
Note that they use a different package to estimate the model and you will see that our estimates are very close, though not exact.&lt;/p&gt;
&lt;p&gt;Kane, L.
&amp;amp; Ashbaugh, A. R.
(2017).
Simple and parallel mediation: A tutorial exploring anxiety sensitivity, sensation seeking, and gender.
&lt;em&gt;The Quantitative Methods for Psychology&lt;/em&gt;, 13(3), 148&amp;ndash;165.
&lt;a href=&#34;doi:10.20982/tqmp.13.3.p148&#34;&gt;doi:10.20982/tqmp.13.3.p148&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;read-in-data&#34;&gt;Read in data&lt;/h2&gt;
&lt;p&gt;Downloaded from the journal &lt;a href=&#34;https://www.tqmp.org/RegularArticles/vol13-3/p148/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tqmp &amp;lt;- read_sav(&amp;quot;p148.sav&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| Variable Name | Description                                 | Notes                                      |
+===============+=============================================+============================================+
| Gender        | Gender                                      | 0 = female, 1= male                        |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| ASI.TOT       | Anxiety sensitivity index total scale score | higher scores mean more anxiety            |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| ASI.PHY       | ASI-3 physical concerns subscale            | higher values means more concerns          |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| ASI.SOC       | ASI-3 social concerns subscale              | higher values means more concerns          |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| ASI.COG       | ASI-3 cognitive concerns subscale           | higher values means more concerns          |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| SSs           | UPPS-P sensation seeking subscale           | higher values means more sensation seeking |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+
| NUs           | UPPS-P negative urgency subscale            | higher values means more negative urgency  |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;+&lt;/p&gt;
&lt;h2 id=&#34;correlation&#34;&gt;Correlation&lt;/h2&gt;
&lt;p&gt;Just to explore the variables and their relationships we look at a correlation table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Pretty correlation table:
tab_02 = tqmp%&amp;gt;% 
  correlate() %&amp;gt;%
  shave(upper = TRUE) %&amp;gt;%
  fashion(decimals = 2, na_print = &amp;quot;—&amp;quot;) 
tab_02 %&amp;gt;% 
  kable()
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; term &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Gender &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; ASI.TOT &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; ASI.PHY &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; ASI.SOC &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; ASI.COG &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; SSs &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; NUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Gender &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ASI.TOT &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.12 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ASI.PHY &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.17 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .89 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ASI.SOC &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.09 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .80 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .54 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ASI.COG &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.05 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .90 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .75 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .57 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; SSs &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .19 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.23 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.27 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.17 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; -.14 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; NUs &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .02 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .38 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .39 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; .13 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; — &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;simple-mediation-model&#34;&gt;Simple Mediation Model&lt;/h1&gt;
&lt;p&gt;This example is from the paper directly.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Independent Variable (X): Gender.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dependent Variable (Y): Sensation seeking (SSs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mediator (M): Anxiety sensitivity (ASI.TOT)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 1.
(from paper)&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/mediation-lab/simple_med_hu3f683187e1d086740fe25db8acab2935_19770_fe77fc0ca6ee50546ee4e6be1d1013b1.JPG 400w,
               /post/mediation-lab/simple_med_hu3f683187e1d086740fe25db8acab2935_19770_8cfc66b73f6840a407abe94310b83234.JPG 760w,
               /post/mediation-lab/simple_med_hu3f683187e1d086740fe25db8acab2935_19770_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://dinaarch.netlify.app/post/mediation-lab/simple_med_hu3f683187e1d086740fe25db8acab2935_19770_fe77fc0ca6ee50546ee4e6be1d1013b1.JPG&#34;
               width=&#34;760&#34;
               height=&#34;247&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;testing-statistical-assumptions&#34;&gt;Testing Statistical Assumptions&lt;/h2&gt;
&lt;p&gt;Before we dive into analyzing the mediation model, we first check the statistical assumptions.
Please follow along on page 151 in the tutorial paper to see how they interpreted the assumptions for this mediation model.&lt;/p&gt;
&lt;h4 id=&#34;linearity-homoscedasticity-normality-of-estimation-error&#34;&gt;Linearity, Homoscedasticity, Normality of estimation error&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Linearity&lt;/em&gt;:&amp;ldquo;To examine these criteria for a simple mediation, you need to plot residuals against predicted values in four regressions: X predicting Y (c), X predicting M (a), M predicting Y (b), and X and M predicting Y (combined linearity of b and c&#39;).&amp;rdquo; (pg. 151)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Homoscedasticity&lt;/em&gt;: &amp;ldquo;To check homoscedasticity, return to the same plot that we created to examine linearity, but this time look for consistency in vertical range across the X axis. In other words, see if the data spreads on the Y axis consistently and equally throughout the plot, resembling a rectangle.&amp;rdquo; (pg. 152)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Normality of estimation error&lt;/em&gt;: &amp;ldquo;To examine this assumption, we can create a Q-Q plot with the residuals we saved from the regression&amp;rdquo; (pg. 152)&lt;/p&gt;
&lt;p&gt;In R, &lt;strong&gt;plot 1&lt;/strong&gt; creates a plot of the residuals against the predicted values and &lt;strong&gt;plot 2&lt;/strong&gt; creates a Q-Q plot of the residuals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# X predicting Y (path c) 
fit_c &amp;lt;- lm(SSs ~ Gender, data=tqmp) 
plot(fit_c, c(1,2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# X predicting M (path a)
fit_a &amp;lt;- lm(ASI.TOT ~ Gender, data=tqmp) 
plot(fit_a, c(1,2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# M predicting Y (path b)
fit_b &amp;lt;- lm(SSs ~ ASI.TOT, data=tqmp) 
plot(fit_b, c(1,2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# X and M predicting Y (b and c&#39;)
fit_cb &amp;lt;- lm(SSs ~  Gender + ASI.TOT, data=tqmp) 
plot(fit_cb, c(1,2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-7.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-4-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;simple-mediation-r-code&#34;&gt;Simple Mediation R Code&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important note!&lt;/strong&gt; There are &lt;strong&gt;two&lt;/strong&gt; &lt;code&gt;mediate()&lt;/code&gt; functions used in this lab.
One comes from the &lt;code&gt;psych&lt;/code&gt; package and the other comes from the &lt;code&gt;mediation&lt;/code&gt; package.
You must identify the package in the code like shown below to specify which &lt;code&gt;mediate()&lt;/code&gt; function you&amp;rsquo;re using.
For simple mediation with one mediator, we use &lt;a href=&#34;https://ademos.people.uic.edu/Chapter14.html#24_method_2:_the_mediation_pacakge_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mediation::mediate()&lt;/code&gt;&lt;/a&gt;.
For multiple mediators, we use &lt;a href=&#34;http://personality-project.org/r/psych/HowTo/mediation.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;psych::mediate()&lt;/code&gt;&lt;/a&gt; shown in the next example.&lt;/p&gt;
&lt;p&gt;Recall the steps to test for a mediation effect using one mediator (See slide 15 in lecture):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Model 1: M regressed on X, path a
fit_a &amp;lt;-  lm(ASI.TOT ~ Gender, data=tqmp) 
summary(fit_a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = ASI.TOT ~ Gender, data = tqmp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.135 -11.343  -3.135  10.657  47.449 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   26.135      1.236  21.151   &amp;lt;2e-16 ***
## Gender        -3.584      1.750  -2.048   0.0415 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.03 on 293 degrees of freedom
## Multiple R-squared:  0.01411,	Adjusted R-squared:  0.01074 
## F-statistic: 4.192 on 1 and 293 DF,  p-value: 0.0415
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Model 2: Y is regressed on X, path c
fit_c &amp;lt;- lm(SSs ~ Gender, data=tqmp) 
summary(fit_c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SSs ~ Gender, data = tqmp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.88379 -0.38379  0.01295  0.44955  1.34628 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  2.65372    0.04778  55.538  &amp;lt; 2e-16 ***
## Gender       0.23007    0.06769   3.399  0.00077 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5813 on 293 degrees of freedom
## Multiple R-squared:  0.03793,	Adjusted R-squared:  0.03465 
## F-statistic: 11.55 on 1 and 293 DF,  p-value: 0.0007702
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Model 3: Y is regressed on X and M (paths c&#39; and b)
fit_cb &amp;lt;- lm(SSs ~ Gender+ ASI.TOT, data=tqmp) 
summary(fit_cb)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SSs ~ Gender + ASI.TOT, data = tqmp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.90432 -0.35627  0.01137  0.43682  1.36129 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  2.864067   0.074417  38.487  &amp;lt; 2e-16 ***
## Gender       0.201224   0.066792   3.013 0.002816 ** 
## ASI.TOT     -0.008049   0.002213  -3.636 0.000327 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5695 on 292 degrees of freedom
## Multiple R-squared:  0.07961,	Adjusted R-squared:  0.07331 
## F-statistic: 12.63 on 2 and 292 DF,  p-value: 5.49e-06
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Path b (not a required step to test for mediation but will be used in the write up)
fit_b &amp;lt;- lm(SSs ~ ASI.TOT, data=tqmp) 
summary(fit_b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SSs ~ ASI.TOT, data = tqmp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.80681 -0.41348  0.00638  0.40406  1.32333 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  2.983622   0.063814  46.755  &amp;lt; 2e-16 ***
## ASI.TOT     -0.008841   0.002228  -3.968 9.11e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5773 on 293 degrees of freedom
## Multiple R-squared:  0.05101,	Adjusted R-squared:  0.04777 
## F-statistic: 15.75 on 1 and 293 DF,  p-value: 9.108e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;test-for-indirect-effect&#34;&gt;Test for Indirect Effect&lt;/h3&gt;
&lt;p&gt;As mentioned in lecture (slide 17), there are multiple ways to estimate the indirect effect and its SE.
The one we will use is the bootstrapped confidence intervals (similar to the tutorial paper as noted on page 153).
In &lt;code&gt;mediation::mediate()&lt;/code&gt;, we enter path a and combined path c and b to test the indirect effect using bootstrapping procedures.
In the tutorial paper, they used 10,000 bootstrap samples (pg 153).
This may take a minute to run.
You can cache the results by using &lt;code&gt;{r, cache = TRUE}&lt;/code&gt; in the top part of the code chunk so it doesn&amp;rsquo;t need to run every time you knit.
You may decrease the number to 1,000 if you find that&amp;rsquo;s taking too long to run (the results may not be identical to the paper, however).&lt;/p&gt;
&lt;p&gt;In the output, we look at the &lt;em&gt;ACME&lt;/em&gt; (or Average Causal Mediation Effect) to evaluate the indirect effect of the mediator.
The &lt;em&gt;ADE&lt;/em&gt; (or Average Direct Effects) provides the direct effect estimate, and the &lt;em&gt;Total Effect&lt;/em&gt; provides the combined indirect and direct effects estimate.
&lt;em&gt;Prop. Mediated&lt;/em&gt; is the ratio of these estimates (which we will not be using).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fitMed &amp;lt;- mediation::mediate(fit_a, fit_cb, treat=&amp;quot;Gender&amp;quot;, mediator=&amp;quot;ASI.TOT&amp;quot;, boot= TRUE,  boot.ci.type = &amp;quot;perc&amp;quot;, sims = 10000)
summary(fitMed)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Causal Mediation Analysis 
## 
## Nonparametric Bootstrap Confidence Intervals with the Percentile Method
## 
##                Estimate 95% CI Lower 95% CI Upper p-value    
## ACME           0.028847     0.000593         0.07  0.0456 *  
## ADE            0.201224     0.069159         0.33  0.0024 ** 
## Total Effect   0.230071     0.099398         0.36  0.0006 ***
## Prop. Mediated 0.125384     0.002647         0.37  0.0462 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Sample Size Used: 295 
## 
## 
## Simulations: 10000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/mediation-lab/simple_med_w_coef_hue9a2cb9288130c77e246a0d43c3f7fdb_22715_4c3e32fc0f06149197ed82978f4ab76c.jpg 400w,
               /post/mediation-lab/simple_med_w_coef_hue9a2cb9288130c77e246a0d43c3f7fdb_22715_0ad02c9a16263722719bc594ce3a7d4b.jpg 760w,
               /post/mediation-lab/simple_med_w_coef_hue9a2cb9288130c77e246a0d43c3f7fdb_22715_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://dinaarch.netlify.app/post/mediation-lab/simple_med_w_coef_hue9a2cb9288130c77e246a0d43c3f7fdb_22715_4c3e32fc0f06149197ed82978f4ab76c.jpg&#34;
               width=&#34;760&#34;
               height=&#34;226&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: This figure was taken from the paper.&lt;/p&gt;
&lt;h3 id=&#34;full-or-partial-mediation&#34;&gt;Full or Partial Mediation?&lt;/h3&gt;
&lt;p&gt;In this example, the direct effect is still significant when accounting the mediator.
We also see that the direct effect with the mediator (c&#39; = 0.201) is less than the total effect (c = 0.230).
Thus, this is evidence of partial mediation (see Slide 10 in lecture).&lt;/p&gt;
&lt;h3 id=&#34;sample-write-up&#34;&gt;Sample Write-Up&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: that the estimates from our estimation of the model here is slightly off from what is reported in the manuscript.
They used the &lt;code&gt;PROCESS&lt;/code&gt;macro and we use the &lt;code&gt;mediate&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Results from a simple mediation analysis indicated that gender is indirectly related to sensation seeking through its relationship with anxiety sensitivity.
First, as can be seen in Figure 7, men reported less anxiety sensitivity than women (&lt;em&gt;a&lt;/em&gt; = −3.584, &lt;em&gt;p&lt;/em&gt; = .042), and lower reported anxiety sensitivity was subsequently related to more sensation seeking (&lt;em&gt;b&lt;/em&gt; = −0.008, &lt;em&gt;p&lt;/em&gt; =&amp;lt; .001).
A 95% bias-corrected confidence interval based on 10,000 bootstrap samples indicated that the indirect effect (&lt;em&gt;ab&lt;/em&gt; = 0.029) was entirely above zero (0.003 to 0.074).
Moreover, men reported greater sensation seeking even after taking into account gender&amp;rsquo;s indirect effect through anxiety sensitivity (&lt;em&gt;c&#39;&lt;/em&gt; = 0.201, &lt;em&gt;p&lt;/em&gt; = .003).&lt;/p&gt;
&lt;h1 id=&#34;complex-mediation-with-multiple-mediators&#34;&gt;Complex Mediation with Multiple Mediators&lt;/h1&gt;
&lt;p&gt;Here, we continue the tutorial paper&amp;rsquo;s example and adding on multiple mediators (three) into the model: Physical, Cognitive, and Social Concerns.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Independent Variable (X): Gender.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dependent Variable (Y): Sensation seeking (SSs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mediator 1 (M1): Physical Concerns (ASI.PHY)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mediator 2 (M2): Social Concerns (ASI.SOC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mediator 3 (M3): Cognitive Concerns (ASI.COG)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/mediation-lab/multiple_med_hucc75820830292ef932686c7d57f04614_19178_78b23fbce9c62d8585ed65081be660e3.JPG 400w,
               /post/mediation-lab/multiple_med_hucc75820830292ef932686c7d57f04614_19178_03aa803d804195cc602443b7a3be9f13.JPG 760w,
               /post/mediation-lab/multiple_med_hucc75820830292ef932686c7d57f04614_19178_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://dinaarch.netlify.app/post/mediation-lab/multiple_med_hucc75820830292ef932686c7d57f04614_19178_78b23fbce9c62d8585ed65081be660e3.JPG&#34;
               width=&#34;538&#34;
               height=&#34;258&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;testing-statistical-assumptions-1&#34;&gt;Testing Statistical Assumptions&lt;/h2&gt;
&lt;p&gt;From the paper: &amp;ldquo;We verified the assumptions using the same methods we used for simple mediation, only this time we conducted seven additional regressions (i.e., X [gender] predicting each mediator [the three anxiety sensitivity dimensions]; each mediator predicting Y [sensation seeking]; X and all three mediators predicting Y ; note: we already had regressed Y on X for the simple mediation).&amp;rdquo; (pg 156).&lt;/p&gt;
&lt;p&gt;We won&amp;rsquo;t run all seven for this lab, but just note that this is what needs to be done with multiple mediators.&lt;/p&gt;
&lt;h2 id=&#34;complex-mediation-r-code&#34;&gt;Complex Mediation R Code&lt;/h2&gt;
&lt;p&gt;We are using the &lt;a href=&#34;http://personality-project.org/r/psych/HowTo/mediation.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;psych::mediate()&lt;/code&gt;&lt;/a&gt; function to specify multiple mediators (you can do this in the other package).
This function will go through the steps of mediation as well as provide the tests of indirect effects for each mediator (three).
An important note (again), the results will only show up when you knit the markdown.
We spent &lt;em&gt;many, many&lt;/em&gt; hours on this only to find that knitting reveals the specific indirect effect estimates and their significance tests.&lt;/p&gt;
&lt;h3 id=&#34;test-for-indirect-effect-1&#34;&gt;Test for Indirect Effect&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fitComplex &amp;lt;- psych::mediate(SSs ~ Gender + (ASI.PHY) + (ASI.SOC)+ (ASI.COG), n.iter = 10000, data = tqmp)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/mediation-lab/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fitComplex
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mediation/Moderation Analysis 
## Call: psych::mediate(y = SSs ~ Gender + (ASI.PHY) + (ASI.SOC) + (ASI.COG), 
##     data = tqmp, n.iter = 10000)
## 
## The DV (Y) was  SSs . The IV (X) was  Gender . The mediating variable(s) =  ASI.PHY ASI.SOC ASI.COG .
## 
## Total effect(c) of  Gender  on  SSs  =  0.23   S.E. =  0.07  t  =  3.4  df=  293   with p =  0.00077
## Direct effect (c&#39;) of  Gender  on  SSs  removing  ASI.PHY ASI.SOC ASI.COG  =  0.17   S.E. =  0.07  t  =  2.57  df=  290   with p =  0.011
## Indirect effect (ab) of  Gender  on  SSs  through  ASI.PHY ASI.SOC ASI.COG   =  0.06 
## Mean bootstrapped indirect effect =  0.06  with standard error =  0.03  Lower CI =  0.01    Upper CI =  0.12
## R = 0.32 R2 = 0.1   F = 8.2 on 4 and 290 DF   p-value:  2.96e-07 
## 
##  To see the longer output, specify short = FALSE in the print statement or ask for the summary
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;sample-write-up-1&#34;&gt;Sample Write-Up&lt;/h3&gt;
&lt;p&gt;Results from a parallel mediation analysis indicated that gender is indirectly related to sensation seeking through its relationship with the Physical Concerns subscale of anxiety sensitivity.
This dimension pertains to the fear of physiological sensations because of the belief that they may have negative consequences and are life-threatening.
First, as can be seen in Figure 9, men reported less fear of physiological sensations than women (&lt;em&gt;a1&lt;/em&gt; = −2.021, &lt;em&gt;p&lt;/em&gt; = .004), and lower reported fear of physiological sensations was subsequently related to more sensation seeking (&lt;em&gt;b1&lt;/em&gt; = −0.029, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001).
A 95% bias-corrected confidence interval based on 10,000 bootstrap samples indicated that the indirect effect through fear of physiological sensations (&lt;em&gt;a1*b1&lt;/em&gt; = 0.058), holding all other mediators constant, was entirely above zero (0.017 to 0.132).
In contrast, the indirect effects through both the Social and the Cognitive Concerns subscales of anxiety sensitivity were not different than zero (−0.004 to 0.038 and −0.047 to 0.005, respectively; see Figure 9 (in the paper) for the effects associated with these pathways).
Moreover, men reported greater sensation seeking even when taking into account gender&amp;rsquo;s indirect effect through all three dimensions of anxiety sensitivity (&lt;em&gt;c&#39;&lt;/em&gt; = 0.172, &lt;em&gt;p&lt;/em&gt; = .011).&lt;/p&gt;
&lt;h2 id=&#34;next-steps-and-resources&#34;&gt;Next steps and resources&lt;/h2&gt;
&lt;p&gt;There are extensions of these models.
It is possible to add moderators to this.
This would allow for the mediatoinal relationships to vary by levels of the moderator.
There are resources out there to do this, which we won&amp;rsquo;t cover here.
My hope is that you will have a foundation in moderation and mediation to extend your own knowledge.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://m-clark.github.io/posts/2019-03-12-mediation-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This&lt;/a&gt; resources was a helpful way to learn about mediation and the different package in R.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tips and Tricks in R</title>
      <link>https://dinaarch.netlify.app/post/tips-and-tricks-in-r/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/post/tips-and-tricks-in-r/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
library(stringr) # str_to_lower()
library(janitor) # clean_names()
library(palmerpenguins) # penguins
library(sjmisc) # move_columns
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s use a dataset called &lt;code&gt;penguins&lt;/code&gt; in the &lt;code&gt;palmerpenguins&lt;/code&gt; package (Reference: &lt;a href=&#34;https://allisonhorst.github.io/palmerpenguins/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Horst AM, Hill AP, &amp;amp; Gorman KB, 2020&lt;/a&gt;)&lt;/p&gt;
&lt;h1 id=&#34;read-in-and-view-data&#34;&gt;Read in and view data&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(&amp;quot;penguins&amp;quot;)
head(penguins_raw) # reads first 6 rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 17
##   studyName `Sample Number` Species       Region Island  Stage   `Individual ID`
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          
## 1 PAL0708                 1 Adelie Pengu~ Anvers Torger~ Adult,~ N1A1           
## 2 PAL0708                 2 Adelie Pengu~ Anvers Torger~ Adult,~ N1A2           
## 3 PAL0708                 3 Adelie Pengu~ Anvers Torger~ Adult,~ N2A1           
## 4 PAL0708                 4 Adelie Pengu~ Anvers Torger~ Adult,~ N2A2           
## 5 PAL0708                 5 Adelie Pengu~ Anvers Torger~ Adult,~ N3A1           
## 6 PAL0708                 6 Adelie Pengu~ Anvers Torger~ Adult,~ N3A2           
## # ... with 10 more variables: Clutch Completion &amp;lt;chr&amp;gt;, Date Egg &amp;lt;date&amp;gt;,
## #   Culmen Length (mm) &amp;lt;dbl&amp;gt;, Culmen Depth (mm) &amp;lt;dbl&amp;gt;,
## #   Flipper Length (mm) &amp;lt;dbl&amp;gt;, Body Mass (g) &amp;lt;dbl&amp;gt;, Sex &amp;lt;chr&amp;gt;,
## #   Delta 15 N (o/oo) &amp;lt;dbl&amp;gt;, Delta 13 C (o/oo) &amp;lt;dbl&amp;gt;, Comments &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(penguins_raw) # lists variable names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;studyName&amp;quot;           &amp;quot;Sample Number&amp;quot;       &amp;quot;Species&amp;quot;            
##  [4] &amp;quot;Region&amp;quot;              &amp;quot;Island&amp;quot;              &amp;quot;Stage&amp;quot;              
##  [7] &amp;quot;Individual ID&amp;quot;       &amp;quot;Clutch Completion&amp;quot;   &amp;quot;Date Egg&amp;quot;           
## [10] &amp;quot;Culmen Length (mm)&amp;quot;  &amp;quot;Culmen Depth (mm)&amp;quot;   &amp;quot;Flipper Length (mm)&amp;quot;
## [13] &amp;quot;Body Mass (g)&amp;quot;       &amp;quot;Sex&amp;quot;                 &amp;quot;Delta 15 N (o/oo)&amp;quot;  
## [16] &amp;quot;Delta 13 C (o/oo)&amp;quot;   &amp;quot;Comments&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(penguins_raw) # provides some descriptive statistics for each variable
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   studyName         Sample Number      Species             Region         
##  Length:344         Min.   :  1.00   Length:344         Length:344        
##  Class :character   1st Qu.: 29.00   Class :character   Class :character  
##  Mode  :character   Median : 58.00   Mode  :character   Mode  :character  
##                     Mean   : 63.15                                        
##                     3rd Qu.: 95.25                                        
##                     Max.   :152.00                                        
##                                                                           
##     Island             Stage           Individual ID      Clutch Completion 
##  Length:344         Length:344         Length:344         Length:344        
##  Class :character   Class :character   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
##                                                                             
##                                                                             
##                                                                             
##                                                                             
##     Date Egg          Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm)
##  Min.   :2007-11-09   Min.   :32.10      Min.   :13.10     Min.   :172.0      
##  1st Qu.:2007-11-28   1st Qu.:39.23      1st Qu.:15.60     1st Qu.:190.0      
##  Median :2008-11-09   Median :44.45      Median :17.30     Median :197.0      
##  Mean   :2008-11-27   Mean   :43.92      Mean   :17.15     Mean   :200.9      
##  3rd Qu.:2009-11-16   3rd Qu.:48.50      3rd Qu.:18.70     3rd Qu.:213.0      
##  Max.   :2009-12-01   Max.   :59.60      Max.   :21.50     Max.   :231.0      
##                       NA&#39;s   :2          NA&#39;s   :2         NA&#39;s   :2          
##  Body Mass (g)      Sex            Delta 15 N (o/oo) Delta 13 C (o/oo)
##  Min.   :2700   Length:344         Min.   : 7.632    Min.   :-27.02   
##  1st Qu.:3550   Class :character   1st Qu.: 8.300    1st Qu.:-26.32   
##  Median :4050   Mode  :character   Median : 8.652    Median :-25.83   
##  Mean   :4202                      Mean   : 8.733    Mean   :-25.69   
##  3rd Qu.:4750                      3rd Qu.: 9.172    3rd Qu.:-25.06   
##  Max.   :6300                      Max.   :10.025    Max.   :-23.79   
##  NA&#39;s   :2                         NA&#39;s   :14        NA&#39;s   :13       
##    Comments        
##  Length:344        
##  Class :character  
##  Mode  :character  
##                    
##                    
##                    
## 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How to read in data and make note of the missing values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data &amp;lt;- read_csv(&amp;quot;data.csv&amp;quot;, na = c(&amp;quot;-999&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;000&amp;quot;)) #note: the `na` argument only works for &amp;quot;read_csv&amp;quot; and not &amp;quot;read_sav.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;clean-names&#34;&gt;Clean names&lt;/h1&gt;
&lt;p&gt;We see that the variable names are not &amp;ldquo;clean.&amp;rdquo; As in, they have spaces and capital letters.
Usually, we want to use a format called &amp;ldquo;snake case&amp;rdquo;, where we have lower case letters and spaces are replaced with underscores.
We can easily do this with the &lt;code&gt;clean_names()&lt;/code&gt; function in the &lt;code&gt;janitor&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_new &amp;lt;- penguins_raw %&amp;gt;% 
  clean_names()

#Compare names
names(penguins_new)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;study_name&amp;quot;        &amp;quot;sample_number&amp;quot;     &amp;quot;species&amp;quot;          
##  [4] &amp;quot;region&amp;quot;            &amp;quot;island&amp;quot;            &amp;quot;stage&amp;quot;            
##  [7] &amp;quot;individual_id&amp;quot;     &amp;quot;clutch_completion&amp;quot; &amp;quot;date_egg&amp;quot;         
## [10] &amp;quot;culmen_length_mm&amp;quot;  &amp;quot;culmen_depth_mm&amp;quot;   &amp;quot;flipper_length_mm&amp;quot;
## [13] &amp;quot;body_mass_g&amp;quot;       &amp;quot;sex&amp;quot;               &amp;quot;delta_15_n_o_oo&amp;quot;  
## [16] &amp;quot;delta_13_c_o_oo&amp;quot;   &amp;quot;comments&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(penguins_raw)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;studyName&amp;quot;           &amp;quot;Sample Number&amp;quot;       &amp;quot;Species&amp;quot;            
##  [4] &amp;quot;Region&amp;quot;              &amp;quot;Island&amp;quot;              &amp;quot;Stage&amp;quot;              
##  [7] &amp;quot;Individual ID&amp;quot;       &amp;quot;Clutch Completion&amp;quot;   &amp;quot;Date Egg&amp;quot;           
## [10] &amp;quot;Culmen Length (mm)&amp;quot;  &amp;quot;Culmen Depth (mm)&amp;quot;   &amp;quot;Flipper Length (mm)&amp;quot;
## [13] &amp;quot;Body Mass (g)&amp;quot;       &amp;quot;Sex&amp;quot;                 &amp;quot;Delta 15 N (o/oo)&amp;quot;  
## [16] &amp;quot;Delta 13 C (o/oo)&amp;quot;   &amp;quot;Comments&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;move-variable-names&#34;&gt;Move variable names&lt;/h1&gt;
&lt;p&gt;We can use the &lt;code&gt;move_columns()&lt;/code&gt; function to rearrange our variable names. Lets say we want to move &lt;code&gt;clutch_completion&lt;/code&gt; to the left of &lt;code&gt;body_mass_g&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_move &amp;lt;- penguins_new %&amp;gt;% 
  move_columns(clutch_completion, .before = body_mass_g)

names(penguins_move)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;study_name&amp;quot;        &amp;quot;sample_number&amp;quot;     &amp;quot;species&amp;quot;          
##  [4] &amp;quot;region&amp;quot;            &amp;quot;island&amp;quot;            &amp;quot;stage&amp;quot;            
##  [7] &amp;quot;individual_id&amp;quot;     &amp;quot;date_egg&amp;quot;          &amp;quot;culmen_length_mm&amp;quot; 
## [10] &amp;quot;culmen_depth_mm&amp;quot;   &amp;quot;flipper_length_mm&amp;quot; &amp;quot;clutch_completion&amp;quot;
## [13] &amp;quot;body_mass_g&amp;quot;       &amp;quot;sex&amp;quot;               &amp;quot;delta_15_n_o_oo&amp;quot;  
## [16] &amp;quot;delta_13_c_o_oo&amp;quot;   &amp;quot;comments&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;recoding&#34;&gt;Recoding&lt;/h1&gt;
&lt;h2 id=&#34;examine-class&#34;&gt;Examine class&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s examine the &lt;code&gt;clutch_completion&lt;/code&gt; variable.
(a character string denoting if the study nest observed with a full clutch, i.e., 2 eggs).
Let&amp;rsquo;s see what the labels look like using &lt;code&gt;unique()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(penguins_new$clutch_completion)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Yes&amp;quot; &amp;quot;No&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Right now, the levels are &lt;code&gt;Yes&lt;/code&gt; and &lt;code&gt;No&lt;/code&gt;.
Let&amp;rsquo;s also check the variable type using &lt;code&gt;class()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_new$clutch_completion)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s make this a factor.&lt;/p&gt;
&lt;h2 id=&#34;create-factor&#34;&gt;Create factor&lt;/h2&gt;
&lt;p&gt;Since this variable is not a character (but a factor), we need to convert it.
We can do that using &lt;code&gt;factor()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_factor &amp;lt;- penguins_new %&amp;gt;% 
  mutate(clutch_factor = factor(clutch_completion))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s check the variable again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_factor$clutch_factor)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(penguins_factor$clutch_factor)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Yes No 
## Levels: No Yes
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;recoding-a-factor&#34;&gt;Recoding a factor&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s say we want to recode our labels from &lt;code&gt;Yes&lt;/code&gt; and &lt;code&gt;No&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_recoded &amp;lt;- penguins_factor %&amp;gt;% 
  mutate(clutch_recoded = recode(clutch_factor,&amp;quot;Yes&amp;quot; = &amp;quot;1&amp;quot;, &amp;quot;No&amp;quot; = &amp;quot;0&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_recoded$clutch_recoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(penguins_recoded$clutch_recoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 0
## Levels: 0 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;recoding-a-character-into-a-factor&#34;&gt;Recoding a character into a factor&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s look at another variable in the &lt;code&gt;penguins&lt;/code&gt; dataset: &lt;code&gt;Sex&lt;/code&gt; of the penguin:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_new$sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(penguins_new$sex)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;MALE&amp;quot;   &amp;quot;FEMALE&amp;quot; NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s say we didn&amp;rsquo;t do the process above, and it was still in &lt;code&gt;character&lt;/code&gt; format.
We can combine those two steps by recoding character variables into factors (from &lt;code&gt;MALE&lt;/code&gt; and &lt;code&gt;FEMALE&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;) using &lt;code&gt;recode_factor():&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_sex_recoded &amp;lt;- penguins_new %&amp;gt;% 
  mutate(sex_recoded = recode_factor(sex,&amp;quot;MALE&amp;quot; = &amp;quot;0&amp;quot;, &amp;quot;FEMALE&amp;quot; = &amp;quot;1&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets check the variable again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_sex_recoded$sex_recoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(penguins_sex_recoded$sex_recoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0    1    &amp;lt;NA&amp;gt;
## Levels: 0 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Side Note&lt;/em&gt;.
I also wanted to make a quick note on what happens when you select the variable of interest first vs. not:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_recoded_select &amp;lt;- penguins_new %&amp;gt;% 
  select(sex) %&amp;gt;% 
  mutate(sex_recoded = recode_factor(sex,&amp;quot;MALE&amp;quot; = &amp;quot;0&amp;quot;, &amp;quot;FEMALE&amp;quot; = &amp;quot;1&amp;quot;))

head(penguins_recoded_select)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   sex    sex_recoded
##   &amp;lt;chr&amp;gt;  &amp;lt;fct&amp;gt;      
## 1 MALE   0          
## 2 FEMALE 1          
## 3 FEMALE 1          
## 4 &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;       
## 5 FEMALE 1          
## 6 MALE   0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(penguins_sex_recoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 18
##   study_name sample_number species         region island  stage    individual_id
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        
## 1 PAL0708                1 Adelie Penguin~ Anvers Torger~ Adult, ~ N1A1         
## 2 PAL0708                2 Adelie Penguin~ Anvers Torger~ Adult, ~ N1A2         
## 3 PAL0708                3 Adelie Penguin~ Anvers Torger~ Adult, ~ N2A1         
## 4 PAL0708                4 Adelie Penguin~ Anvers Torger~ Adult, ~ N2A2         
## 5 PAL0708                5 Adelie Penguin~ Anvers Torger~ Adult, ~ N3A1         
## 6 PAL0708                6 Adelie Penguin~ Anvers Torger~ Adult, ~ N3A2         
## # ... with 11 more variables: clutch_completion &amp;lt;chr&amp;gt;, date_egg &amp;lt;date&amp;gt;,
## #   culmen_length_mm &amp;lt;dbl&amp;gt;, culmen_depth_mm &amp;lt;dbl&amp;gt;, flipper_length_mm &amp;lt;dbl&amp;gt;,
## #   body_mass_g &amp;lt;dbl&amp;gt;, sex &amp;lt;chr&amp;gt;, delta_15_n_o_oo &amp;lt;dbl&amp;gt;, delta_13_c_o_oo &amp;lt;dbl&amp;gt;,
## #   comments &amp;lt;chr&amp;gt;, sex_recoded &amp;lt;fct&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;recoding-multiple-variables&#34;&gt;Recoding multiple variables&lt;/h2&gt;
&lt;p&gt;The dataset doesn&amp;rsquo;t have any two variables that need to be recoded the same, but let&amp;rsquo;s recode both &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;clutch_completion&lt;/code&gt; at the same time. &lt;em&gt;Not&lt;/em&gt;e: this code should be used if you have two variables that need to have the same recoding, but it looks like this works too.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_multiple &amp;lt;- penguins_new %&amp;gt;%  
    mutate_at(c(&amp;quot;sex&amp;quot;, &amp;quot;clutch_completion&amp;quot;),
              ~ recode_factor(., &amp;quot;MALE&amp;quot; = &amp;quot;0&amp;quot;, &amp;quot;FEMALE&amp;quot; = &amp;quot;1&amp;quot;,
                       &amp;quot;Yes&amp;quot; = &amp;quot;1&amp;quot;, &amp;quot;No&amp;quot; = &amp;quot;0&amp;quot;))
penguins_multiple %&amp;gt;% 
  select(sex, clutch_completion) %&amp;gt;% 
  head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   sex   clutch_completion
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;            
## 1 0     1                
## 2 1     1                
## 3 1     1                
## 4 &amp;lt;NA&amp;gt;  1                
## 5 1     1                
## 6 0     1
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;creating-reference-variables&#34;&gt;Creating reference variables&lt;/h1&gt;
&lt;p&gt;Recall slide 33 in the Multiple Regression lecture:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/tips-and-tricks-in-r/dummycoding_hu6cbf3d693b4dad74d88547ca3f98430f_134779_9222e6dcd2d39aba170cda4a5370ce61.jpg 400w,
               /post/tips-and-tricks-in-r/dummycoding_hu6cbf3d693b4dad74d88547ca3f98430f_134779_ff1368a0288b4524db3a174e53301500.jpg 760w,
               /post/tips-and-tricks-in-r/dummycoding_hu6cbf3d693b4dad74d88547ca3f98430f_134779_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://dinaarch.netlify.app/post/tips-and-tricks-in-r/dummycoding_hu6cbf3d693b4dad74d88547ca3f98430f_134779_9222e6dcd2d39aba170cda4a5370ce61.jpg&#34;
               width=&#34;760&#34;
               height=&#34;430&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Remember that when we enter a categorical variable into the model using &lt;code&gt;lm()&lt;/code&gt;, R creates the reference variable for us, and picks the first category as the reference category.
For example, let&amp;rsquo;s look at the &lt;code&gt;species&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_new$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(penguins_new$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Adelie Penguin (Pygoscelis adeliae)&amp;quot;      
## [2] &amp;quot;Gentoo penguin (Pygoscelis papua)&amp;quot;        
## [3] &amp;quot;Chinstrap penguin (Pygoscelis antarctica)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s clean this variable.
First, let&amp;rsquo;s make the labels lowercase using &lt;code&gt;str_to_lower()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguin_species &amp;lt;- penguins_new %&amp;gt;% 
  mutate(species = str_to_lower(species))

unique(penguin_species$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;adelie penguin (pygoscelis adeliae)&amp;quot;      
## [2] &amp;quot;gentoo penguin (pygoscelis papua)&amp;quot;        
## [3] &amp;quot;chinstrap penguin (pygoscelis antarctica)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s just keep the simple names of the penguins: &lt;code&gt;adelie&lt;/code&gt;, &lt;code&gt;gentoo&lt;/code&gt;, and &lt;code&gt;chinstrap&lt;/code&gt; using the &lt;code&gt;separate()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_species_clean &amp;lt;- penguin_species %&amp;gt;% 
  separate(species, into = c(&amp;quot;species&amp;quot;, &amp;quot;delete&amp;quot;), sep = &amp;quot; penguin&amp;quot;) %&amp;gt;% 
  select(-delete)

unique(penguins_species_clean$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;adelie&amp;quot;    &amp;quot;gentoo&amp;quot;    &amp;quot;chinstrap&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_species_clean$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s also convert this new variable into a factor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_species_clean$species &amp;lt;- factor(penguins_species_clean$species)

unique(penguins_species_clean$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] adelie    gentoo    chinstrap
## Levels: adelie chinstrap gentoo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;class(penguins_species_clean$species)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s quickly run a simple linear regression using the &lt;code&gt;species&lt;/code&gt; variable without creating the reference variables.
Let&amp;rsquo;s predict &lt;code&gt;flipper_length_mm&lt;/code&gt; from &lt;code&gt;species&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model_1 &amp;lt;- lm(flipper_length_mm ~ species, data = penguins_species_clean)
summary(model_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = flipper_length_mm ~ species, data = penguins_species_clean)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.9536  -4.8235   0.0464   4.8130  20.0464 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)      189.9536     0.5405 351.454  &amp;lt; 2e-16 ***
## specieschinstrap   5.8699     0.9699   6.052 3.79e-09 ***
## speciesgentoo     27.2333     0.8067  33.760  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.642 on 339 degrees of freedom
##   (2 observations deleted due to missingness)
## Multiple R-squared:  0.7782,	Adjusted R-squared:  0.7769 
## F-statistic: 594.8 on 2 and 339 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, it automatically created the reference variables and made &lt;code&gt;adelie&lt;/code&gt; penguins the reference category.
What if we want to compare our results to &lt;code&gt;gentoo&lt;/code&gt; penguins?
We can do that by converting the &lt;code&gt;chinstrap&lt;/code&gt; and &lt;code&gt;adelie&lt;/code&gt; into their own variables using &lt;code&gt;ifelse()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chinstrap &amp;lt;- ifelse(penguins_species_clean$species == &#39;chinstrap&#39;, 1, 0)
adelie &amp;lt;- ifelse(penguins_species_clean$species == &#39;adelie&#39;, 1, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we add them to our dataset using &lt;code&gt;cbind()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_reference &amp;lt;- penguins_species_clean %&amp;gt;% 
  cbind(chinstrap, adelie)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s look at the model again with &lt;code&gt;gentoo&lt;/code&gt; as the reference variable.
Instead of &lt;code&gt;species&lt;/code&gt;, we use &lt;code&gt;chinstrap&lt;/code&gt; and &lt;code&gt;adelie&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model_2 &amp;lt;- lm(flipper_length_mm ~ chinstrap + adelie, data = penguins_reference)
summary(model_2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = flipper_length_mm ~ chinstrap + adelie, data = penguins_reference)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.9536  -4.8235   0.0464   4.8130  20.0464 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 217.1870     0.5988  362.68   &amp;lt;2e-16 ***
## chinstrap   -21.3635     1.0036  -21.29   &amp;lt;2e-16 ***
## adelie      -27.2333     0.8067  -33.76   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.642 on 339 degrees of freedom
##   (2 observations deleted due to missingness)
## Multiple R-squared:  0.7782,	Adjusted R-squared:  0.7769 
## F-statistic: 594.8 on 2 and 339 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Changing the reference category does NOT change our model.
It only allows us to make a more meaningful comparison.&lt;/p&gt;
&lt;h1 id=&#34;collapsing-variables&#34;&gt;Collapsing variables&lt;/h1&gt;
&lt;h2 id=&#34;continous-to-categorical&#34;&gt;Continous to categorical&lt;/h2&gt;
&lt;p&gt;In lab 1, I go over how to collapse continuous variables into categorical. Let&amp;rsquo;s categorize &lt;code&gt;body_mass_g&lt;/code&gt; into &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;3&lt;/code&gt;, and &lt;code&gt;4&lt;/code&gt; levels.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(penguins_new$body_mass_g)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##    2700    3550    4050    4202    4750    6300       2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cont_cat &amp;lt;- penguins_new %&amp;gt;% 
  mutate(body_mass_factor = cut(body_mass_g,
                      breaks = c(2700, 3550, 4202, 4750, 6300),
                      labels = c(&amp;quot;low&amp;quot;, &amp;quot;medium_low&amp;quot;, &amp;quot;medium_high&amp;quot;, &amp;quot;high&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;categorical-to-categorical&#34;&gt;Categorical to categorical&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s collapse our &lt;code&gt;body_mass_g&lt;/code&gt; even further. This code is useful if you have a Likert-type scale with four categories, such as &lt;code&gt;Strong Agree&lt;/code&gt; to &lt;code&gt;Strongly Disagree&lt;/code&gt; and want to collapse to a category with those who responded &lt;code&gt;Strongly Agree&lt;/code&gt; and &lt;code&gt;Agree&lt;/code&gt; as &lt;code&gt;1&lt;/code&gt;, and &lt;code&gt;Disagree&lt;/code&gt; and &lt;code&gt;Strongly Disagree&lt;/code&gt; as &lt;code&gt;2&lt;/code&gt;. We can use &lt;code&gt;fct_collapse&lt;/code&gt; to collapse factor levels into groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cat_cat &amp;lt;- cont_cat %&amp;gt;% 
  mutate(body_mass_collpase = 
           fct_collapse(body_mass_factor,
                        low = c(&amp;quot;low&amp;quot;, &amp;quot;medium_low&amp;quot;),
                        high = c(&amp;quot;medium_high&amp;quot;, &amp;quot;high&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Binary Logistic Regression Lab</title>
      <link>https://dinaarch.netlify.app/post/binary-logistic-regression-lab/</link>
      <pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/post/binary-logistic-regression-lab/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(pander) #pander()
library(psych) # describe()
library(gtsummary) #tbl_summary()
library(equatiomatic) # extract_eq()
library(sjPlot) # tab_xtab(), tab_model()
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This lab follows along the logistic regression lecture presented in class and provides two examples to demonstrate the models and their interpretation.&lt;/p&gt;
&lt;h1 id=&#34;what-is-binary-logistic-regression&#34;&gt;What is Binary Logistic Regression?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is a regression with an &lt;strong&gt;outcome&lt;/strong&gt; &lt;strong&gt;variable&lt;/strong&gt; (or &lt;strong&gt;dependent variable&lt;/strong&gt;) that is dichotomous/binary (i.e., only two categories, such as Yes or No, 0 or 1, Disorder or No Disorder, Win or Lose).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;predictor variables&lt;/strong&gt; (or &lt;strong&gt;independent variables&lt;/strong&gt; or &lt;strong&gt;explanatory variables&lt;/strong&gt;) can be either continuous or categorical&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In binary logistic regression, we are interested in predicting which of two possible events (e.g., win or lose) are going to happen given the predictor(s) variables&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assumes a binomial distribution or “a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions.” (&lt;a href=&#34;https://www.investopedia.com/terms/b/binomialdistribution.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;binary-logistic-regression-in-r&#34;&gt;Binary Logistic Regression in R&lt;/h1&gt;
&lt;h2 id=&#34;admissions-example&#34;&gt;Admissions Example&lt;/h2&gt;
&lt;p&gt;This example comes from the UCLA Statistical Consulting page found &lt;a href=&#34;https://stats.idre.ucla.edu/r/dae/logit-regression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
This example assesses how GRE, GPA, and prestige of the undergraduate institutions effect graduate school admission.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Variable&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;gre&lt;/td&gt;
&lt;td&gt;Graduate Record Exam (GRE) scores, &lt;em&gt;continuous&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gpa&lt;/td&gt;
&lt;td&gt;Grade point average, &lt;em&gt;continuous&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;rank&lt;/td&gt;
&lt;td&gt;Rank of undergraduate institutions, 1 = highest prestige, 4 = lowest, &lt;em&gt;categorical&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;admit&lt;/td&gt;
&lt;td&gt;Admission decision, 0 = did not admit, 1 = admitted, &lt;em&gt;binary&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hosmer, D.
&amp;amp; Lemeshow, S.
(2000).
Applied Logistic Regression (Second Edition).
New York: John Wiley &amp;amp; Sons, Inc.&lt;/p&gt;
&lt;p&gt;Long, J. Scott (1997).
Regression Models for Categorical and Limited Dependent Variables.
Thousand Oaks, CA: Sage Publications.&lt;/p&gt;
&lt;h3 id=&#34;read-in-data&#34;&gt;Read in data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;log &amp;lt;- read.csv(&amp;quot;https://stats.idre.ucla.edu/stat/data/binary.csv&amp;quot;) %&amp;gt;% 
  mutate(rank = factor(rank),  #we need to convert this variable to a factor
         admit = factor(admit, levels = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;), labels = c(&amp;quot;Not Admitted&amp;quot;, &amp;quot;Admitted&amp;quot;))) # add labels to outcome
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# psych::describe()
round(describe(log),2) %&amp;gt;% 
  pander() 
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt; &lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;vars&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;n&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;mean&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;sd&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;median&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;trimmed&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;mad&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;min&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;admit&lt;/strong&gt;*&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.32&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.47&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.27&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;gre&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;587.7&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;115.5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;580&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;589.1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;118.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;gpa&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3.39&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.38&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3.4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3.4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;rank&lt;/strong&gt;*&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.48&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.94&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.48&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.48&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table continues below&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt; &lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;max&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;range&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;skew&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;kurtosis&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;se&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;admit&lt;/strong&gt;*&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.78&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-1.39&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;gre&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;800&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;580&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.36&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;gpa&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.74&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.21&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;rank&lt;/strong&gt;*&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# I like this table more, but you can do either! Especially if you&#39;re getting some errors with gtsummary(). 

# gtsummary::tbl_summary()
tbl_summary(log,
            statistic = list(all_continuous() ~ &amp;quot;{mean} ({sd})&amp;quot;),
                             missing = &amp;quot;no&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;rnvuxdypxd&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}
&lt;p&gt;#rnvuxdypxd .gt_table {
display: table;
border-collapse: collapse;
margin-left: auto;
margin-right: auto;
color: #333333;
font-size: 16px;
font-weight: normal;
font-style: normal;
background-color: #FFFFFF;
width: auto;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #A8A8A8;
border-right-style: none;
border-right-width: 2px;
border-right-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #A8A8A8;
border-left-style: none;
border-left-width: 2px;
border-left-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_heading {
background-color: #FFFFFF;
text-align: center;
border-bottom-color: #FFFFFF;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_title {
color: #333333;
font-size: 125%;
font-weight: initial;
padding-top: 4px;
padding-bottom: 4px;
border-bottom-color: #FFFFFF;
border-bottom-width: 0;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_subtitle {
color: #333333;
font-size: 85%;
font-weight: initial;
padding-top: 0;
padding-bottom: 6px;
border-top-color: #FFFFFF;
border-top-width: 0;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_bottom_border {
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_col_headings {
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_col_heading {
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: normal;
text-transform: inherit;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
vertical-align: bottom;
padding-top: 5px;
padding-bottom: 6px;
padding-left: 5px;
padding-right: 5px;
overflow-x: hidden;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_column_spanner_outer {
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: normal;
text-transform: inherit;
padding-top: 0;
padding-bottom: 0;
padding-left: 4px;
padding-right: 4px;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_column_spanner_outer:first-child {
padding-left: 0;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_column_spanner_outer:last-child {
padding-right: 0;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_column_spanner {
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
vertical-align: bottom;
padding-top: 5px;
padding-bottom: 5px;
overflow-x: hidden;
display: inline-block;
width: 100%;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_group_heading {
padding: 8px;
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: initial;
text-transform: inherit;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
vertical-align: middle;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_empty_group_heading {
padding: 0.5px;
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: initial;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
vertical-align: middle;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_from_md &amp;gt; :first-child {
margin-top: 0;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_from_md &amp;gt; :last-child {
margin-bottom: 0;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_row {
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
margin: 10px;
border-top-style: solid;
border-top-width: 1px;
border-top-color: #D3D3D3;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
vertical-align: middle;
overflow-x: hidden;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_stub {
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: initial;
text-transform: inherit;
border-right-style: solid;
border-right-width: 2px;
border-right-color: #D3D3D3;
padding-left: 12px;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_summary_row {
color: #333333;
background-color: #FFFFFF;
text-transform: inherit;
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_first_summary_row {
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_grand_summary_row {
color: #333333;
background-color: #FFFFFF;
text-transform: inherit;
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_first_grand_summary_row {
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
border-top-style: double;
border-top-width: 6px;
border-top-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_striped {
background-color: rgba(128, 128, 128, 0.05);
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_table_body {
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_footnotes {
color: #333333;
background-color: #FFFFFF;
border-bottom-style: none;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 2px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 2px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_footnote {
margin: 0px;
font-size: 90%;
padding: 4px;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_sourcenotes {
color: #333333;
background-color: #FFFFFF;
border-bottom-style: none;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 2px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 2px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_sourcenote {
font-size: 90%;
padding: 4px;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_left {
text-align: left;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_center {
text-align: center;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_right {
text-align: right;
font-variant-numeric: tabular-nums;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_font_normal {
font-weight: normal;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_font_bold {
font-weight: bold;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_font_italic {
font-style: italic;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_super {
font-size: 65%;
}&lt;/p&gt;
&lt;p&gt;#rnvuxdypxd .gt_footnote_marks {
font-style: italic;
font-weight: normal;
font-size: 65%;
}
&lt;/style&gt;&lt;/p&gt;
&lt;table class=&#34;gt_table&#34;&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;&lt;strong&gt;Characteristic&lt;/strong&gt;&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;&lt;strong&gt;N = 400&lt;/strong&gt;&lt;sup class=&#34;gt_footnote_marks&#34;&gt;1&lt;/sup&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;admit&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34; style=&#34;text-align: left; text-indent: 10px;&#34;&gt;Not Admitted&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;273 (68%)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34; style=&#34;text-align: left; text-indent: 10px;&#34;&gt;Admitted&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;127 (32%)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;gre&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;588 (116)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;gpa&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;3.39 (0.38)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;rank&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34; style=&#34;text-align: left; text-indent: 10px;&#34;&gt;1&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;61 (15%)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34; style=&#34;text-align: left; text-indent: 10px;&#34;&gt;2&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;151 (38%)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34; style=&#34;text-align: left; text-indent: 10px;&#34;&gt;3&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;121 (30%)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34; style=&#34;text-align: left; text-indent: 10px;&#34;&gt;4&lt;/td&gt;
&lt;td class=&#34;gt_row gt_center&#34;&gt;67 (17%)&lt;/td&gt;&lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tfoot&gt;
    &lt;tr class=&#34;gt_footnotes&#34;&gt;
      &lt;td colspan=&#34;2&#34;&gt;
        &lt;p class=&#34;gt_footnote&#34;&gt;
          &lt;sup class=&#34;gt_footnote_marks&#34;&gt;
            &lt;em&gt;1&lt;/em&gt;
          &lt;/sup&gt;
&lt;pre&gt;&lt;code&gt;      n (%); Mean (SD)
      &amp;lt;br /&amp;gt;
    &amp;lt;/p&amp;gt;
  &amp;lt;/td&amp;gt;
&amp;lt;/tr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;contingency-table&#34;&gt;Contingency Table&lt;/h3&gt;
&lt;p&gt;Contingency tables, sometimes called cross tabulations or cross tabs, that displays the frequency distribution of your &lt;strong&gt;categorical&lt;/strong&gt; variables.
In R, we use &lt;code&gt;sjPlot::tab_xtabs()&lt;/code&gt; (Found by Karen!).
We want to check the contingency table to make sure there are values in each cell.
Since we only have one categorical variable, &lt;code&gt;rank&lt;/code&gt;, we only need to check one contingency table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sjPlot::tab_xtab(var.row = log$rank, var.col = log$admit, title = &amp;quot;Cross tab of Admit rate by Rank of Undergraduate Insitute&amp;quot;, show.row.prc = TRUE, show.summary=F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;border-collapse:collapse; border:none;&#34;&gt;
&lt;caption style=&#34;font-weight: bold; text-align:left;&#34;&gt;
Cross tab of Admit rate by Rank of Undergraduate Insitute
&lt;/caption&gt;
&lt;tr&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal; border-bottom:1px solid;&#34; rowspan=&#34;2&#34;&gt;
rank
&lt;/th&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal;&#34; colspan=&#34;2&#34;&gt;
admit
&lt;/th&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal; font-weight:bolder; font-style:italic; border-bottom:1px solid; &#34; rowspan=&#34;2&#34;&gt;
Total
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom:1px solid; text-align:center; padding:0.2cm;&#34;&gt;
Not Admitted
&lt;/td&gt;
&lt;td style=&#34;border-bottom:1px solid; text-align:center; padding:0.2cm;&#34;&gt;
Admitted
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;45.9 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;54.1 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;64.2 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;35.8 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;151&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;93&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;76.9 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;23.1 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;121&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;82.1 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;17.9 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  border-bottom:double; font-weight:bolder; font-style:italic; text-align:left; vertical-align:middle;&#34;&gt;
Total
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;273&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;68.2 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;127&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;31.8 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;400&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This table shows the row and column totals, as well as row percentages.
You can also ask for column percentages, if you prefer.&lt;/p&gt;
&lt;h3 id=&#34;binary-logistic-model&#34;&gt;Binary Logistic Model&lt;/h3&gt;
&lt;p&gt;Recall in linear regression, we used the &lt;code&gt;lm()&lt;/code&gt; function.
With logistic regression, we use &lt;code&gt;glm()&lt;/code&gt;, or general linear model.
The set up is similar to linear regression, however.
One important note is to make sure we identify the type of “family function,” or the description of the error distribution used in the model.
For logistic regression, it’s &lt;code&gt;family = &amp;quot;binomial&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In our example, the predictors are GRE scores, GPA, and university rank related to graduate school admission status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mylogit &amp;lt;- glm(admit ~ gre + gpa + rank, data = log, family = &amp;quot;binomial&amp;quot;)
summary(mylogit)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = admit ~ gre + gpa + rank, family = &amp;quot;binomial&amp;quot;, 
##     data = log)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6268  -0.8662  -0.6388   1.1490   2.0790  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -3.989979   1.139951  -3.500 0.000465 ***
## gre          0.002264   0.001094   2.070 0.038465 *  
## gpa          0.804038   0.331819   2.423 0.015388 *  
## rank2       -0.675443   0.316490  -2.134 0.032829 *  
## rank3       -1.340204   0.345306  -3.881 0.000104 ***
## rank4       -1.551464   0.417832  -3.713 0.000205 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.52
## 
## Number of Fisher Scoring iterations: 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s dissect this output.
It looks similar to regular simple linear regression but interpretation is different (as we saw in lecture).
When looking at the coefficients, we see &lt;code&gt;gre&lt;/code&gt;, &lt;code&gt;gpa&lt;/code&gt;, and the levels of &lt;code&gt;rank&lt;/code&gt; are significant predictors of admission status.&lt;/p&gt;
&lt;p&gt;With binary logistic regression, regression coefficient is the change in the log odds of the outcome for a one unit increase in the predictor variable, holding others constant.
The intercept is the log odds for students who has zero on all the predictors, a value not observed in this sample (e.g., no one has GPA or GRE score of zero).&lt;/p&gt;
&lt;p&gt;So this is how we would interpret those same variables when running a logistic regression model (Note: it’s a z-statistic now since these are standardized coefficients):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GRE was significantly related to admission decision (&lt;em&gt;z&lt;/em&gt; = 2.070, &lt;em&gt;p&lt;/em&gt; = 0.038). Specifically, the results indicate that for every one-unit increase in GRE score, there is an increase of &lt;em&gt;B&lt;/em&gt; = 0.002 in the &lt;strong&gt;the log odds&lt;/strong&gt; of being admitted to graduate school, controlling for GPA and university rank.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, this is how we would interpret the categorical variable &lt;code&gt;rank&lt;/code&gt;.
Note that because this is a categorical variable, we are comparing the categories to a comparison group (or a reference group).
In this example, the comparison group is &lt;strong&gt;rank 1&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Students who attended rank 2 universities are less likely to be admitted to graduate school, compared to rank 1 universities (&lt;em&gt;z&lt;/em&gt; = -0.675, &lt;em&gt;p&lt;/em&gt; = 0.033).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The log odds, or &lt;em&gt;logits&lt;/em&gt;, are not always easily interpreted and can be converted to odds ratios for easier interpretation.&lt;/p&gt;
&lt;h3 id=&#34;odds-ratios&#34;&gt;Odds Ratios&lt;/h3&gt;
&lt;p&gt;The coefficients presented above are log odds.
To convert them to odd ratios, we take the exponent of the coefficients (Slide 21 in lecture):&lt;/p&gt;
&lt;p&gt;$$
odds ratio = e^{\beta}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#coverting the log odds (logits) to odds ratios
exp(coef(mylogit))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)         gre         gpa       rank2       rank3       rank4 
##   0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    OR       2.5 %    97.5 %
## (Intercept) 0.0185001 0.001889165 0.1665354
## gre         1.0022670 1.000137602 1.0044457
## gpa         2.2345448 1.173858216 4.3238349
## rank2       0.5089310 0.272289674 0.9448343
## rank3       0.2617923 0.131641717 0.5115181
## rank4       0.2119375 0.090715546 0.4706961
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These values likely make &lt;em&gt;a little more&lt;/em&gt; sense.
Considering GRE in the example again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GRE was significantly related to admission decision (&lt;strong&gt;z&lt;/strong&gt; = 2.070, &lt;em&gt;p&lt;/em&gt; = 0.038). Specifically, for every one-unit increase in GRE score, &lt;strong&gt;the odds&lt;/strong&gt; &lt;strong&gt;of being admitted to graduate school increase by a factor of 1.002&lt;/strong&gt;, controlling for GPA and university rank. (Note this is a very small amount because one point increase in GRE doesn’t represent a large difference).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Said differently,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GRE was significantly related to admission decision (&lt;strong&gt;z&lt;/strong&gt; = 2.070, &lt;em&gt;p&lt;/em&gt; = 0.038). Specifically, for every one-unit increase in GRE score, &lt;strong&gt;we expect to see about a .002% increase in the odds of being admitted to graduate school&lt;/strong&gt;, controlling for GPA and university rank.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;predicted-probability&#34;&gt;Predicted Probability&lt;/h3&gt;
&lt;p&gt;To make the coefficients easier to interpret, we can take the odds ratio values and convert them to probabilities.
To do this, we take the odds ratio and divide it by the odds ratio plus 1 (See slide 25):&lt;/p&gt;
&lt;p&gt;$$
probability = \frac{odds}{odds + 1}
$$&lt;/p&gt;
&lt;p&gt;Before we dive right into conversions, we could also calculate the &lt;strong&gt;predicted probabilities&lt;/strong&gt; of admission for specific values of the the predictors.
Recall in lecture (Slide 23) to find the log odds of an x-value, we want to plug it into our regression equation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;extract_eq(mylogit, use_coefs = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$$
\log\left[ \frac { \widehat{P( \operatorname{admit} = \operatorname{Admitted} )} }{ 1 - \widehat{P( \operatorname{admit} = \operatorname{Admitted} )} } \right] = -3.99 + 0(\operatorname{gre}) + 0.8(\operatorname{gpa}) - 0.68(\operatorname{rank}&lt;em&gt;{\operatorname{2}}) - 1.34(\operatorname{rank}&lt;/em&gt;{\operatorname{3}}) - 1.55(\operatorname{rank}_{\operatorname{4}})
$$&lt;/p&gt;
&lt;p&gt;To create predicted probabilities, we need to create a new data frame with the x-value we want the independent variables to take on to create our predictions (In lecture, slide 23, the value was &lt;em&gt;SurvRate&lt;/em&gt; = 40).
For this example, let’s use the mean.
And since we have a &lt;strong&gt;categorical&lt;/strong&gt; predictor, &lt;code&gt;rank&lt;/code&gt;, lets look at the mean of &lt;code&gt;gre&lt;/code&gt; and &lt;code&gt;gpa&lt;/code&gt; and each value of university &lt;code&gt;rank&lt;/code&gt;.
We could, of course, do all the calculations by hand.
But R is easier :)&lt;/p&gt;
&lt;p&gt;Let’s first create the data frame using &lt;code&gt;with&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pred_prob &amp;lt;- with(log, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
pred_prob
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     gre    gpa rank
## 1 587.7 3.3899    1
## 2 587.7 3.3899    2
## 3 587.7 3.3899    3
## 4 587.7 3.3899    4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have our mean of &lt;code&gt;gpa&lt;/code&gt; and &lt;code&gt;gre&lt;/code&gt; and each level of &lt;code&gt;rank&lt;/code&gt;.
Which means we are going to get &lt;em&gt;four&lt;/em&gt; predicted probabilities, at the mean of the other two variables.
If we did not have a categorical predictor, we would only have &lt;em&gt;one.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now let’s create the predicted probabilities using the mean of &lt;code&gt;gre&lt;/code&gt; and &lt;code&gt;gpa&lt;/code&gt; and each level of &lt;code&gt;rank&lt;/code&gt; using the &lt;code&gt;predict()&lt;/code&gt; function.
What R is doing here is taking the logits from &lt;code&gt;mylogit&lt;/code&gt; and converting them to predicted probabilities based on the values we provided in &lt;code&gt;pred_prob&lt;/code&gt;.
We identify the object as &lt;code&gt;pred_prob$rankP&lt;/code&gt; to create a new column of the within the &lt;code&gt;pred_prob&lt;/code&gt; dataset called &lt;code&gt;rankP&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pred_prob$rankP &amp;lt;- predict(mylogit, newdata = pred_prob, type = &amp;quot;response&amp;quot;)
pred_prob
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     gre    gpa rank     rankP
## 1 587.7 3.3899    1 0.5166016
## 2 587.7 3.3899    2 0.3522846
## 3 587.7 3.3899    3 0.2186120
## 4 587.7 3.3899    4 0.1846684
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great!
Now we have predicted probabilities.
Here is our interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For those with an average GPA and GRE score (holding GPA and GRE constant), the predicted probabilities of being accepted into a graduate program is .516 for those in the highest ranked undergraduate institutions (rank = 1) and .184 for students from the lowest ranked undergraduate institutions (rank=4). This demonstrates the relation of rank of the university in the probability of being admitted to graduate school.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;classification-table&#34;&gt;Classification Table&lt;/h3&gt;
&lt;p&gt;A classification table is similar to a contingency table, but in this table we compare the actual values to the predicted values based on the binary logistic regression model.
This is a way to see how well the model did at predicting the outcome of being admitted to graduate school.&lt;/p&gt;
&lt;p&gt;In this code, we take actual fitted values from the model (&lt;code&gt;mylogit$fitted.values&lt;/code&gt;), and separate them by a cut off of &lt;em&gt;p&lt;/em&gt; = .5.
Those who had a fitted value of above .50 were assigned “Admitted.” We are comparing the values given by the model (fitted values) and whether or not they actually got admitted.
The purpose of this is to see how effective our model is at predicting admission status&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Converting from probability to actual output
log$fitted_admit &amp;lt;- ifelse(mylogit$fitted.values &amp;gt;= 0.5, &amp;quot;1&amp;quot;, &amp;quot;0&amp;quot;) #1 = Admitted, 0 = Not Admitted

# Generating the classification table
ctab &amp;lt;- table(log$admit, log$fitted_admit)
ctab
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               
##                  0   1
##   Not Admitted 254  19
##   Admitted      97  30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#I (Karen) like this table better because it include row and column totals
sjPlot::tab_xtab(var.row = log$admit, var.col = log$fitted_admit, title = &amp;quot;Observed Admission versus Predited Admission counts&amp;quot;, show.row.prc = TRUE, show.summary = F )
&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;border-collapse:collapse; border:none;&#34;&gt;
&lt;caption style=&#34;font-weight: bold; text-align:left;&#34;&gt;
Observed Admission versus Predited Admission counts
&lt;/caption&gt;
&lt;tr&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal; border-bottom:1px solid;&#34; rowspan=&#34;2&#34;&gt;
admit
&lt;/th&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal;&#34; colspan=&#34;2&#34;&gt;
fitted_admit
&lt;/th&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal; font-weight:bolder; font-style:italic; border-bottom:1px solid; &#34; rowspan=&#34;2&#34;&gt;
Total
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom:1px solid; text-align:center; padding:0.2cm;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom:1px solid; text-align:center; padding:0.2cm;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
Not Admitted
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;254&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;93 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;7 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;273&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
Admitted
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;76.4 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;23.6 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;127&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  border-bottom:double; font-weight:bolder; font-style:italic; text-align:left; vertical-align:middle;&#34;&gt;
Total
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;351&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;87.8 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;12.2 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;400&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Here, we can see that the model correctly predicted 30 of 127, or 23.6%, of the students that actually got admitted.
Thus, it misclassified 76.4% (97) of the admitted students were predicted to be “not admitted.” Additionally, we see that it correctly classified 254 of 273 (93%) of the not admitted students.
Thus this model did a better job at predicting who was not admitted than those who were admitted.&lt;/p&gt;
&lt;h3 id=&#34;accuracy&#34;&gt;Accuracy&lt;/h3&gt;
&lt;p&gt;We can check the accuracy percentage:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;accuracy &amp;lt;- sum(diag(ctab))/sum(ctab)*100
accuracy
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall, the accuracy of our model is 71%, or our logistic model is able to classify 71% of the observations correctly.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-model-building-with-logistic-regression&#34;&gt;Hierarchical model building with logistic regression&lt;/h2&gt;
&lt;p&gt;Just as we did in linear regression, we can proceed through a model building process with logistic regression.
In this context, we are interested in seeing how we can improve the prediction of the outcome in a hierarchical fashion.&lt;/p&gt;
&lt;p&gt;Let’s say we fit the following models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Predicting admission from just GPA (log1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Predicting admission from GPA and GRE (log2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Predicting admission using GPA, GRE, and rank (log3)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can create a table of all three models using &lt;code&gt;tab_model&lt;/code&gt; of the &lt;code&gt;sjPlot&lt;/code&gt; package.
We can compare the R2 estimates across the models.
Using the &lt;code&gt;anova&lt;/code&gt;function, we can also test if there is a statistical improvement in fit by adding the predictor(s).
In logistic regression models, we test if the residual deviance significant decreases.
We are using the ANOVA function, but it’s not an ANOVA test like you may have done before.
In this case, we are using ANOVA to compare the deviance for each model to the subsequent model.
If the chi-square value is significant we can interpret that the addition of the extra variables (going from log1 to log 2, for example) significantly improves the model prediction (e.g., lowers the residual variance).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;log1 &amp;lt;- glm(admit ~ gpa , data = log, family = &amp;quot;binomial&amp;quot;)
log2 &amp;lt;- glm(admit ~ gpa + gre , data = log, family = &amp;quot;binomial&amp;quot;)
log3 &amp;lt;- glm(admit ~ gpa + gre + rank, data = log, family = &amp;quot;binomial&amp;quot;)

tab_model(log1, log2, log3)
&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;border-collapse:collapse; border:none;&#34;&gt;
&lt;tr&gt;
&lt;th style=&#34;border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; &#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; &#34;&gt;
admit
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; &#34;&gt;
admit
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; &#34;&gt;
admit
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; &#34;&gt;
Predictors
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  &#34;&gt;
Odds Ratios
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  &#34;&gt;
CI
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  &#34;&gt;
p
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  &#34;&gt;
Odds Ratios
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  &#34;&gt;
CI
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col7&#34;&gt;
p
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col8&#34;&gt;
Odds Ratios
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col9&#34;&gt;
CI
&lt;/td&gt;
&lt;td style=&#34; text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  0&#34;&gt;
p
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; &#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
0.00 – 0.09
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;strong&gt;\&lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
0.00 – 0.06
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7&#34;&gt;
&lt;strong&gt;\&lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9&#34;&gt;
0.00 – 0.17
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0&#34;&gt;
&lt;strong&gt;\&lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; &#34;&gt;
gpa
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
2.86
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
1.61 – 5.20
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;strong&gt;\&lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
2.13
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
1.14 – 4.02
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7&#34;&gt;
&lt;strong&gt;0.018&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8&#34;&gt;
2.23
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9&#34;&gt;
1.17 – 4.32
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0&#34;&gt;
&lt;strong&gt;0.015&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; &#34;&gt;
gre
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
1.00 – 1.00
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7&#34;&gt;
&lt;strong&gt;0.011&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9&#34;&gt;
1.00 – 1.00
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0&#34;&gt;
&lt;strong&gt;0.038&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; &#34;&gt;
rank \[2\]
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8&#34;&gt;
0.51
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9&#34;&gt;
0.27 – 0.94
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0&#34;&gt;
&lt;strong&gt;0.033&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; &#34;&gt;
rank \[3\]
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8&#34;&gt;
0.26
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9&#34;&gt;
0.13 – 0.51
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0&#34;&gt;
&lt;strong&gt;\&lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; &#34;&gt;
rank \[4\]
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  &#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8&#34;&gt;
0.21
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9&#34;&gt;
0.09 – 0.47
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0&#34;&gt;
&lt;strong&gt;\&lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;&#34;&gt;
Observations
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;&#34; colspan=&#34;3&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;&#34; colspan=&#34;3&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;&#34; colspan=&#34;3&#34;&gt;
400
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;&#34;&gt;
R&lt;sup&gt;2&lt;/sup&gt; Tjur
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;&#34; colspan=&#34;3&#34;&gt;
0.032
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;&#34; colspan=&#34;3&#34;&gt;
0.047
&lt;/td&gt;
&lt;td style=&#34; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;&#34; colspan=&#34;3&#34;&gt;
0.102
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;anova(log1, log2, log3, test=&amp;quot;Chisq&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Deviance Table
## 
## Model 1: admit ~ gpa
## Model 2: admit ~ gpa + gre
## Model 3: admit ~ gpa + gre + rank
##   Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    
## 1       398     486.97                          
## 2       397     480.34  1   6.6236   0.01006 *  
## 3       394     458.52  3  21.8265 7.088e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the ANOVA results, we see that the addition of each additional predictor signficnalty improves the prediction of graduate school admission.
Note, you don’t have to do this, but if you have research questions about the specific predictive power, controloing for the others, you can use this apporach.&lt;/p&gt;
&lt;h2 id=&#34;the-api-example&#34;&gt;The API Example&lt;/h2&gt;
&lt;p&gt;Data from &lt;a href=&#34;https://stats.idre.ucla.edu/r/seminars/introduction-to-regression-in-r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UCLA - Academic Performance Index&lt;/a&gt;.
Note: This is the same example as the simple linear regression lab (Lab 5).
However, we are changing the model to have a binary outcome for logistic regression.
Below is the list of variables we used in this analysis for lab, noting that there are more variables in the API dataset.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Variable&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;api00&lt;/td&gt;
&lt;td&gt;Academic Performance Index, 0 = under statewide target of 800, 1 = above 800, &lt;em&gt;binary&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enroll&lt;/td&gt;
&lt;td&gt;School enrollment, &lt;em&gt;continuous&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Meals&lt;/td&gt;
&lt;td&gt;Number of free and reduced lunch, &lt;em&gt;continuous&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;full&lt;/td&gt;
&lt;td&gt;Percentage of teachers will full credentials, &lt;em&gt;continuous&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;read-in-data-1&#34;&gt;Read in data&lt;/h3&gt;
&lt;p&gt;Here, we are reading in the data directly from the UCLA website.
Additionally, we are dichotomizing the API (&lt;code&gt;api00&lt;/code&gt;) variable to separate schools above statewide target of 800 and below.
This cut-off may be outdated and/or controversial, however, we are going to use this for the purpose of providing an example of how to do logistic regression in R.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;api &amp;lt;- read.csv(
&amp;quot;https://stats.idre.ucla.edu/wp-content/uploads/2019/02/elemapi2v2.csv&amp;quot;) %&amp;gt;% 
  select(api00, full, enroll, meals) %&amp;gt;%
  mutate(api_factor = cut(api00,
                     levels = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;),
                     breaks = c(0, 800, 940),
                     labels = c(&amp;quot;Below Target&amp;quot;, &amp;quot;At or Above Target&amp;quot;))) # here we are dichotomizing API, see lab 1 for a refresher on what this all does.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;summary-1&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;For a short descriptor of each variable, scroll down &lt;a href=&#34;https://stats.idre.ucla.edu/spss/webbooks/reg/chapter1/regressionwith-spsschapter-1-simple-and-multiple-regression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# psych::describe()
round(describe(api),2) %&amp;gt;% 
  pander() 
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt; &lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;vars&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;n&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;mean&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;sd&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;median&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;trimmed&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;mad&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;min&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;api00&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;647.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;142.2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;643&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;645.8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;177.2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;369&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;full&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;84.55&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14.95&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;88&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14.83&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;enroll&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;483.5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;226.4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;435&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;459.4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;202.4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;130&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;meals&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;60.31&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;31.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;67.5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;62.18&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.81&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;api_factor&lt;/strong&gt;*&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.19&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.39&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table continues below&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt; &lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;max&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;range&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;skew&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;kurtosis&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;se&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;api00&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;940&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;571&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-1.13&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;full&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;100&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;63&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.97&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.17&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.75&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;enroll&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1570&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1440&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.34&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3.02&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;meals&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;100&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;100&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-0.41&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-1.2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;api_factor&lt;/strong&gt;*&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.55&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.42&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# I like this table more, but you can do either! Especially if you&#39;re getting some errors with gtsummary(). Useful when you have a subset, but here we left all variables in the sample so it&#39;s a long table. 

# gtsummary::gtsummary()
#tbl_summary(api,
#            statistic = list(all_continuous() ~ &amp;quot;{mean} ({sd})&amp;quot;),
#                             missing = &amp;quot;no&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no contingency table in this example because all of our predictors are continuous.&lt;/p&gt;
&lt;h3 id=&#34;binary-logistic-model-1&#34;&gt;Binary Logistic Model&lt;/h3&gt;
&lt;p&gt;We are using a binary logistic regression model to see how school characteristics relate to a school making adequate progress (api=1) or not (api=0).&lt;/p&gt;
&lt;p&gt;Binary outcome: API adequate progress =1, not adequate progress=0 Predictors: % fully credential teachers, student enrollment, and % on free and reduced lunch&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mylogit &amp;lt;- glm(api_factor ~ full + enroll + meals, data = api, family = &amp;quot;binomial&amp;quot;)
summary(mylogit)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = api_factor ~ full + enroll + meals, family = &amp;quot;binomial&amp;quot;, 
##     data = api)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.86557  -0.16221  -0.03269  -0.01028   3.12996  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -3.0746093  2.7320279  -1.125   0.2604    
## full         0.0582641  0.0282187   2.065   0.0389 *  
## enroll       0.0004592  0.0013843   0.332   0.7401    
## meals       -0.1096184  0.0144185  -7.603  2.9e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 391.86  on 399  degrees of freedom
## Residual deviance: 147.45  on 396  degrees of freedom
## AIC: 155.45
## 
## Number of Fisher Scoring iterations: 8
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;odds-ratios-1&#34;&gt;Odds Ratios&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;exp(coef(mylogit))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)        full      enroll       meals 
##  0.04620768  1.05999495  1.00045932  0.89617604
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     OR        2.5 %    97.5 %
## (Intercept) 0.04620768 0.0001656294 7.8435280
## full        1.05999495 1.0055761469 1.1237801
## enroll      1.00045932 0.9978836793 1.0032716
## meals       0.89617604 0.8684201733 0.9192615
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;predicted-probability-1&#34;&gt;Predicted Probability&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#creating the means of each variable
pred_prob_api&amp;lt;- with(api, data.frame(full = mean(full), enroll = mean(enroll), meals = mean(meals)))
pred_prob_api
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    full  enroll  meals
## 1 84.55 483.465 60.315
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pred_prob_api$apiP&amp;lt;- predict(mylogit, newdata = pred_prob_api, type = &amp;quot;response&amp;quot;)
pred_prob_api
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    full  enroll  meals       apiP
## 1 84.55 483.465 60.315 0.01058167
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This result says that for schools that have average on all the predictors, the probablity of being at target is .01.
&lt;em&gt;yikes!&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;classification-table-1&#34;&gt;Classification Table&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Converting from probability to actual output
api$fitted_api &amp;lt;- ifelse(mylogit$fitted.values &amp;gt;= 0.5, &amp;quot;1&amp;quot;, &amp;quot;0&amp;quot;) #1 = Above Average, 0 = Below

# Generating the classification table
ctab &amp;lt;- table(api$api_factor, api$fitted_api)
ctab
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     
##                        0   1
##   Below Target       304  19
##   At or Above Target  15  62
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#I (Karen) like this table better because it include row and column totals
sjPlot::tab_xtab(var.row = api$api_factor, var.col = api$fitted_api, title = &amp;quot;Observed Progress versus Predited progress counts&amp;quot;, show.row.prc = TRUE, show.summary = F )
&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;border-collapse:collapse; border:none;&#34;&gt;
&lt;caption style=&#34;font-weight: bold; text-align:left;&#34;&gt;
Observed Progress versus Predited progress counts
&lt;/caption&gt;
&lt;tr&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal; border-bottom:1px solid;&#34; rowspan=&#34;2&#34;&gt;
api_factor
&lt;/th&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal;&#34; colspan=&#34;2&#34;&gt;
fitted_api
&lt;/th&gt;
&lt;th style=&#34;border-top:double; text-align:center; font-style:italic; font-weight:normal; font-weight:bolder; font-style:italic; border-bottom:1px solid; &#34; rowspan=&#34;2&#34;&gt;
Total
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom:1px solid; text-align:center; padding:0.2cm;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom:1px solid; text-align:center; padding:0.2cm;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
Below Target
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;304&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;94.1 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;5.9 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;323&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  text-align:left; vertical-align:middle;&#34;&gt;
At or Above Target
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;19.5 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center; &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;80.5 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;  &#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding:0.2cm;  border-bottom:double; font-weight:bolder; font-style:italic; text-align:left; vertical-align:middle;&#34;&gt;
Total
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;319&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;79.8 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;20.2 %&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;padding:0.2cm; text-align:center;   border-bottom:double;&#34;&gt;
&lt;span style=&#34;color:black;&#34;&gt;400&lt;/span&gt;&lt;br&gt;&lt;span style=&#34;color:#333399;&#34;&gt;100 %&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;accuracy-1&#34;&gt;Accuracy&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;accuracy &amp;lt;- sum(diag(ctab))/sum(ctab)*100
accuracy
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.5
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sample-write-up&#34;&gt;Sample write up&lt;/h2&gt;
&lt;p&gt;Using a binary logistic regression, we explored how school’s characteristics (% fully credential teachers, studnet enrollment, and % on free and reduced lunch) relate to the adequate progress of a school.
Both the percent of credentialed teachers (&lt;em&gt;z&lt;/em&gt; = 2.06, &lt;em&gt;p&lt;/em&gt; = .04) and the % free and reduced lunch (&lt;em&gt;z&lt;/em&gt; = -.11, &lt;em&gt;p&lt;/em&gt; &amp;lt; .01) were significant predictors.
Specifically the percent of fully credentialed teachers has an &lt;em&gt;OR&lt;/em&gt; = 1.05, 95%CI[1.01, 1.12].
This implies that for a one percent increase in the fully credentialed teachers, schools have a 5% higher chance of making target.
Free and reduced lunch had estimated &lt;em&gt;OR&lt;/em&gt; =.90, 95% CI [.86, .92], which indicated that for each additional percent of students on free and reduced lunch, there is a significant decrease in the log odds of being at target (&lt;em&gt;z&lt;/em&gt; = -7.60, &lt;em&gt;p&lt;/em&gt; &amp;lt; .01).
That is, for each additional percent increase of free or reduced lunch, that school has a 11% decrease in the odds of making it to target growth, controlling for the other predictors.
There was no effect of the enrollment of the school, when controlling for the other variables.
Overall, this model had high prediction accuracy, predicting 91% schools accurately.&lt;/p&gt;
&lt;p&gt;Hope this is useful….&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning - Final Project</title>
      <link>https://dinaarch.netlify.app/post/machine-learning-final-project/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/post/machine-learning-final-project/</guid>
      <description>&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;p&gt;The U.S. presidential election in 2012 did not come as a surprise. Some correctly predicted the outcome of the election correctly including &lt;a href=&#34;https://en.wikipedia.org/wiki/Nate_Silver&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nate Silver&lt;/a&gt;, 
and &lt;a href=&#34;https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many speculated about his approach&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Despite the success in 2012, the 2016 presidential election came as a 
&lt;a href=&#34;https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;big surprise&lt;/a&gt; 
to many, and it underscored that predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets.&lt;/p&gt;
&lt;p&gt;Your final project will be to merge census data with 2016 voting data to analyze the election outcome.&lt;/p&gt;
&lt;p&gt;To familiarize yourself with the general problem of predicting election outcomes, read the articles linked above and answer the following questions. Limit your responses to one paragraph for each.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What makes voter behavior prediction (and thus election forecasting) a hard problem?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;**In general, predicting behavior has its limitations. As mentioned in the fivethirtyeight.com article, there is always going to be statistical noise and nonresponse bias to pre-election polls which is then not representative of voter turnout. For the 2016 election specifically, there may have been people who had a preference for a certain candidate during pre-election polls and perhaps changed their mind at the voting booth. **&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;What was unique to Nate Silver&amp;rsquo;s approach in 2012 that allowed him to achieve good predictions?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;The Guardian mentions modeling voting behavior and polls, the mathematical model being: Proportion of people saying they will vote for Obama is that actual percentage + the house effect + sampling variation. Based on Wikipedia&amp;rsquo;s discussion, he published an interactive webpage that allowed readers to predictor the outcome of the election based on three variables: President Obama&amp;rsquo;s favorability ratings, the rate of GDP growth, and how conservative the Republication opponent would be. It&amp;rsquo;s doesn&amp;rsquo;t mention if he used the responses in his model, however.&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;What went wrong in 2016? What do you think should be done to make future predictions better?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;As mentioned before there was high error, such as statistical noise and nonresponse bias. There could be many reasons for this, one example the article mentioned was that pollsters could have been embarrassed to admit who they were supporting. They also could have had a changed their mind about a candidate between the pre-election polls and when voting.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;project_data.RData&lt;/code&gt; binary file contains three datasets: tract-level 2010 census data, stored as &lt;code&gt;census&lt;/code&gt;; metadata &lt;code&gt;census_meta&lt;/code&gt; with variable descriptions and types; and county-level vote tallies from the 2016 election, stored as &lt;code&gt;election_raw&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;election-data&#34;&gt;Election data&lt;/h2&gt;
&lt;p&gt;Some example rows of the election data are shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   county             fips  candidate       state   votes
##   &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Los Angeles County 6037  Hillary Clinton CA    2464364
## 2 Los Angeles County 6037  Donald Trump    CA     769743
## 3 Los Angeles County 6037  Gary Johnson    CA      88968
## 4 Los Angeles County 6037  Jill Stein      CA      76465
## 5 Los Angeles County 6037  Gloria La Riva  CA      21993
## 6 Cook County        17031 Hillary Clinton IL    1611946
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The meaning of each column in &lt;code&gt;election_raw&lt;/code&gt; is self-evident except &lt;code&gt;fips&lt;/code&gt;. The accronym is short for &lt;a href=&#34;https://en.wikipedia.org/wiki/FIPS_county_code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federal Information Processing Standard&lt;/a&gt;. In this dataset, &lt;code&gt;fips&lt;/code&gt; values denote the area (nationwide, statewide, or countywide) that each row of data represent.&lt;/p&gt;
&lt;p&gt;Nationwide and statewide tallies are included as rows in &lt;code&gt;election_raw&lt;/code&gt; with &lt;code&gt;county&lt;/code&gt; values of &lt;code&gt;NA&lt;/code&gt;. There are two kinds of these summary rows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Federal-level summary rows have a &lt;code&gt;fips&lt;/code&gt; value of &lt;code&gt;US&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;State-level summary rows have the state name as the &lt;code&gt;fips&lt;/code&gt; value.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Inspect rows with &lt;code&gt;fips=2000&lt;/code&gt;. Provide a reason for excluding them. Drop these observations &amp;ndash; please write over &lt;code&gt;election_raw&lt;/code&gt; &amp;ndash; and report the data dimensions after removal.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   county fips  candidate          state  votes
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 &amp;lt;NA&amp;gt;   2000  Donald Trump       AK    163387
## 2 &amp;lt;NA&amp;gt;   2000  Hillary Clinton    AK    116454
## 3 &amp;lt;NA&amp;gt;   2000  Gary Johnson       AK     18725
## 4 &amp;lt;NA&amp;gt;   2000  Jill Stein         AK      5735
## 5 &amp;lt;NA&amp;gt;   2000  Darrell Castle     AK      3866
## 6 &amp;lt;NA&amp;gt;   2000  Rocky De La Fuente AK      1240
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The only reason I see for exclusion is lack of county information and the &lt;code&gt;fips&lt;/code&gt; value is identified as neither federal level (&lt;code&gt;fips&lt;/code&gt; value of US) or state-level (&lt;code&gt;fips&lt;/code&gt; value as the state).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dimensions:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18345     5
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;census-data&#34;&gt;Census data&lt;/h2&gt;
&lt;p&gt;The first few rows and columns of the &lt;code&gt;census&lt;/code&gt; data are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   CensusTract State   County  TotalPop   Men Women
##         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1  1001020100 Alabama Autauga     1948   940  1008
## 2  1001020200 Alabama Autauga     2156  1059  1097
## 3  1001020300 Alabama Autauga     2968  1364  1604
## 4  1001020400 Alabama Autauga     4423  2172  2251
## 5  1001020500 Alabama Autauga    10763  4922  5841
## 6  1001020600 Alabama Autauga     3851  1787  2064
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Variable descriptions are given in the &lt;code&gt;metadata&lt;/code&gt; file. The variables shown above are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   variable       description                     type       
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                           &amp;lt;chr&amp;gt;      
## 1 &amp;quot;CensusTract &amp;quot; &amp;quot; Census tract ID &amp;quot;             &amp;quot; numeric &amp;quot;
## 2 &amp;quot;State &amp;quot;       &amp;quot; State, DC, or Puerto Rico &amp;quot;   &amp;quot; string &amp;quot; 
## 3 &amp;quot;County &amp;quot;      &amp;quot; County or county equivalent &amp;quot; &amp;quot; string &amp;quot; 
## 4 &amp;quot;TotalPop &amp;quot;    &amp;quot; Total population &amp;quot;            &amp;quot; numeric &amp;quot;
## 5 &amp;quot;Men &amp;quot;         &amp;quot; Number of men &amp;quot;               &amp;quot; numeric &amp;quot;
## 6 &amp;quot;Women &amp;quot;       &amp;quot; Number of women &amp;quot;             &amp;quot; numeric &amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;\newpage&lt;/p&gt;
&lt;h2 id=&#34;data-preprocessing&#34;&gt;Data preprocessing&lt;/h2&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;
&lt;p&gt;Separate the rows of &lt;code&gt;election_raw&lt;/code&gt; into separate federal-, state-, and county-level data frames:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Store federal-level tallies as &lt;code&gt;election_federal&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Store state-level tallies as &lt;code&gt;election_state&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Store county-level tallies as &lt;code&gt;election&lt;/code&gt;. Coerce the &lt;code&gt;fips&lt;/code&gt; variable to numeric.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Federal-Level Tallies (First 5 Rows):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   county fips  candidate       state    votes
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 &amp;lt;NA&amp;gt;   US    Donald Trump    US    62984825
## 2 &amp;lt;NA&amp;gt;   US    Hillary Clinton US    65853516
## 3 &amp;lt;NA&amp;gt;   US    Gary Johnson    US     4489221
## 4 &amp;lt;NA&amp;gt;   US    Jill Stein      US     1429596
## 5 &amp;lt;NA&amp;gt;   US    Evan McMullin   US      510002
## 6 &amp;lt;NA&amp;gt;   US    Darrell Castle  US      186545
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;State-Level Tallies (First 5 Rows):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   county fips  candidate       state   votes
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 &amp;lt;NA&amp;gt;   CA    Hillary Clinton CA    8753788
## 2 &amp;lt;NA&amp;gt;   CA    Donald Trump    CA    4483810
## 3 &amp;lt;NA&amp;gt;   CA    Gary Johnson    CA     478500
## 4 &amp;lt;NA&amp;gt;   CA    Jill Stein      CA     278657
## 5 &amp;lt;NA&amp;gt;   CA    Gloria La Riva  CA      66101
## 6 &amp;lt;NA&amp;gt;   FL    Donald Trump    FL    4617886
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;County-Level Tallies (First 5 Rows):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   county              fips candidate       state   votes
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Los Angeles County  6037 Hillary Clinton CA    2464364
## 2 Los Angeles County  6037 Donald Trump    CA     769743
## 3 Los Angeles County  6037 Gary Johnson    CA      88968
## 4 Los Angeles County  6037 Jill Stein      CA      76465
## 5 Los Angeles County  6037 Gloria La Riva  CA      21993
## 6 Cook County        17031 Hillary Clinton IL    1611946
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;How many named presidential candidates were there in the 2016 election? Draw a bar graph of all votes received by each candidate, and order the candidate names by decreasing vote counts. (You may need to log-transform the vote axis.)&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
**There are 31 named presidential candidates**
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;Create &lt;code&gt;county_winner&lt;/code&gt; and &lt;code&gt;state_winner&lt;/code&gt; by taking the candidate with the highest proportion of votes. (Hint: to create &lt;code&gt;county_winner&lt;/code&gt;, start with &lt;code&gt;election&lt;/code&gt;, group by &lt;code&gt;fips&lt;/code&gt;, compute &lt;code&gt;total&lt;/code&gt; votes, and &lt;code&gt;pct = votes/total&lt;/code&gt;. Then choose the highest row using &lt;code&gt;slice_max&lt;/code&gt; (variable &lt;code&gt;state_winner&lt;/code&gt; is similar).)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;County Winner (First 5 Counties):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
## # Groups:   fips [6]
##   county          fips candidate       state votes total   pct
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Autauga County  1001 Donald Trump    AL    18172 24759 0.734
## 2 Baldwin County  1003 Donald Trump    AL    72883 94261 0.773
## 3 Barbour County  1005 Donald Trump    AL     5454 10436 0.523
## 4 Bibb County     1007 Donald Trump    AL     6738  8753 0.770
## 5 Blount County   1009 Donald Trump    AL    22859 25442 0.898
## 6 Bullock County  1011 Hillary Clinton AL     3530  4702 0.751
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;State Winner (First 5 State):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
## # Groups:   fips [6]
##   county fips  candidate       state   votes    total   pct
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 &amp;lt;NA&amp;gt;   46102 Hillary Clinton SD       2510     2905 0.864
## 2 &amp;lt;NA&amp;gt;   AK    Donald Trump    AK     163387   309407 0.528
## 3 &amp;lt;NA&amp;gt;   AL    Donald Trump    AL    1318255  2101660 0.627
## 4 &amp;lt;NA&amp;gt;   AR    Donald Trump    AR     684872  1130635 0.606
## 5 &amp;lt;NA&amp;gt;   AZ    Donald Trump    AZ    1252401  2554240 0.490
## 6 &amp;lt;NA&amp;gt;   CA    Hillary Clinton CA    8753788 14060856 0.623
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;visualization&#34;&gt;Visualization&lt;/h1&gt;
&lt;p&gt;Here you&amp;rsquo;ll generate maps of the election data using &lt;code&gt;ggmap&lt;/code&gt;. The .Rmd file for this document contains codes to generate the following map.&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Draw a county-level map with &lt;code&gt;map_data(&amp;quot;county&amp;quot;)&lt;/code&gt; and color by county.&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;In order to map the winning candidate for each state, the map data (&lt;code&gt;states&lt;/code&gt;) must be merged with with the election data (&lt;code&gt;state_winner&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;left_join()&lt;/code&gt; will do the trick, but needs to join the data frames on a variable with values that match. In this case, that variable is the state name, but abbreviations are used in one data frame and the full name is used in the other.&lt;/p&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Use the following function to create a &lt;code&gt;fips&lt;/code&gt; variable in the &lt;code&gt;states&lt;/code&gt; data frame with values that match the &lt;code&gt;fips&lt;/code&gt; variable in &lt;code&gt;election_federal&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First 5 rows in the &lt;code&gt;states&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##        long      lat group order  region subregion fips
## 1 -87.46201 30.38968     1     1 alabama      &amp;lt;NA&amp;gt;   AL
## 2 -87.48493 30.37249     1     2 alabama      &amp;lt;NA&amp;gt;   AL
## 3 -87.52503 30.37249     1     3 alabama      &amp;lt;NA&amp;gt;   AL
## 4 -87.53076 30.33239     1     4 alabama      &amp;lt;NA&amp;gt;   AL
## 5 -87.57087 30.32665     1     5 alabama      &amp;lt;NA&amp;gt;   AL
## 6 -87.58806 30.32665     1     6 alabama      &amp;lt;NA&amp;gt;   AL
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now the data frames can be merged. &lt;code&gt;left_join(df1, df2)&lt;/code&gt; takes all the rows from &lt;code&gt;df1&lt;/code&gt; and looks for matches in &lt;code&gt;df2&lt;/code&gt;. For each match, &lt;code&gt;left_join()&lt;/code&gt; appends the data from the second table to the matching row in the first; if no matching value is found, it adds missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Use &lt;code&gt;left_join&lt;/code&gt; to merge the tables and use the result to create a map of the election results by state. Your figure will look similar to this state level &lt;a href=&#34;https://www.nytimes.com/elections/results/president&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York Times map&lt;/a&gt;. (Hint: use &lt;code&gt;scale_fill_brewer(palette=&amp;quot;Set1&amp;quot;)&lt;/code&gt; for a red-and-blue map.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Merged states by &lt;code&gt;fips&lt;/code&gt; (First 5 rows):
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Now create a county-level map. The county-level map data does not have a &lt;code&gt;fips&lt;/code&gt; value, so to create one, use information from &lt;code&gt;maps::county.fips&lt;/code&gt;: split the &lt;code&gt;polyname&lt;/code&gt; column to &lt;code&gt;region&lt;/code&gt; and &lt;code&gt;subregion&lt;/code&gt; using &lt;code&gt;tidyr::separate&lt;/code&gt;, and use &lt;code&gt;left_join()&lt;/code&gt; to combine &lt;code&gt;county.fips&lt;/code&gt; with the county-level map data. Then construct the map. Your figure will look similar to county-level &lt;a href=&#34;https://www.nytimes.com/elections/results/president&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York Times map&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;ol start=&#34;12&#34;&gt;
&lt;li&gt;Create a visualization of your choice using &lt;code&gt;census&lt;/code&gt; data. Many exit polls noted that &lt;a href=&#34;https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;demographics played a big role in the election&lt;/a&gt;. If you need a starting point, use &lt;a href=&#34;https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this Washington Post article&lt;/a&gt; and &lt;a href=&#34;https://www.r-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this R graph gallery&lt;/a&gt; for ideas and inspiration.&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;ol start=&#34;13&#34;&gt;
&lt;li&gt;The &lt;code&gt;census&lt;/code&gt; data contains high resolution information (more fine-grained than county-level). Aggregate the information into county-level data by computing population-weighted averages of each attribute for each county by carrying out the following steps:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Clean census data, saving the result as &lt;code&gt;census_del&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filter out any rows of &lt;code&gt;census&lt;/code&gt; with missing values;&lt;/li&gt;
&lt;li&gt;convert &lt;code&gt;Men&lt;/code&gt;, &lt;code&gt;Employed&lt;/code&gt;, and &lt;code&gt;Citizen&lt;/code&gt; to percentages;&lt;/li&gt;
&lt;li&gt;compute a &lt;code&gt;Minority&lt;/code&gt; variable by combining &lt;code&gt;Hispanic&lt;/code&gt;, &lt;code&gt;Black&lt;/code&gt;, &lt;code&gt;Native&lt;/code&gt;, &lt;code&gt;Asian&lt;/code&gt;, &lt;code&gt;Pacific&lt;/code&gt;, and remove these variables after creating &lt;code&gt;Minority&lt;/code&gt;; and&lt;/li&gt;
&lt;li&gt;remove &lt;code&gt;Walk&lt;/code&gt;, &lt;code&gt;PublicWork&lt;/code&gt;, and &lt;code&gt;Construction&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create population weights for sub-county census data, saving the result as &lt;code&gt;census_subct&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;group &lt;code&gt;census_del&lt;/code&gt; by &lt;code&gt;State&lt;/code&gt; and &lt;code&gt;County&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;add_tally()&lt;/code&gt; to compute &lt;code&gt;CountyPop&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;compute the population weight as &lt;code&gt;TotalPop/CountyTotal&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;adjust all quantitative variables by multiplying by the population weights.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aggregate census data to county level, &lt;code&gt;census_ct&lt;/code&gt;: group the sub-county data &lt;code&gt;census_subct&lt;/code&gt; by state and county and compute popluation-weighted averages of each variable by taking the sum (since the variables were already transformed by the population weights)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Print the first few rows and columns of &lt;code&gt;census_ct&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 28
## # Groups:   State [1]
##   State   County    Men Women White Citizen Income IncomeErr IncomePerCap
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Alabama Autauga  48.4 3349.  75.8    73.7 51696.     7771.       24974.
## 2 Alabama Baldwin  48.8 3934.  83.1    75.7 51074.     8745.       27317.
## 3 Alabama Barbour  53.8 1492.  46.2    76.9 32959.     6031.       16824.
## 4 Alabama Bibb     53.4 2930.  74.5    77.4 38887.     5662.       18431.
## 5 Alabama Blount   49.4 3562.  87.9    73.4 46238.     8696.       20532.
## 6 Alabama Bullock  53.0 1968.  22.2    75.5 33293.     9000.       17580.
## # ... with 19 more variables: IncomePerCapErr &amp;lt;dbl&amp;gt;, Poverty &amp;lt;dbl&amp;gt;,
## #   ChildPoverty &amp;lt;dbl&amp;gt;, Professional &amp;lt;dbl&amp;gt;, Service &amp;lt;dbl&amp;gt;, Office &amp;lt;dbl&amp;gt;,
## #   Production &amp;lt;dbl&amp;gt;, Drive &amp;lt;dbl&amp;gt;, Carpool &amp;lt;dbl&amp;gt;, Transit &amp;lt;dbl&amp;gt;,
## #   OtherTransp &amp;lt;dbl&amp;gt;, WorkAtHome &amp;lt;dbl&amp;gt;, MeanCommute &amp;lt;dbl&amp;gt;, Employed &amp;lt;dbl&amp;gt;,
## #   PrivateWork &amp;lt;dbl&amp;gt;, SelfEmployed &amp;lt;dbl&amp;gt;, FamilyWork &amp;lt;dbl&amp;gt;,
## #   Unemployment &amp;lt;dbl&amp;gt;, Minority &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;14&#34;&gt;
&lt;li&gt;If you were physically located in the United States on election day for the 2016 presidential election, what state and county were you in? Compare and contrast the results and demographic information for this county with the state it is located in. If you were not in the United States on election day, select any county. Do you find anything unusual or surprising? If so, explain; if not, explain why not.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;On election day, I was in Los Angeles, CA. Taking the average values of some of the variables to look at overall California demographics, it looks like minorities make up 70% of LA, and 40% of California. Men and Unemployement is unchanged. Having lived here a long time, I am not surprised at the demographics. LA is much more diverse than the rest of California.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 28
## # Groups:   State [1]
##   State      County        Men Women White Citizen Income IncomeErr IncomePerCap
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 California Los Angeles  49.2 2460.  26.9    60.1 61385.    10572.       28390.
## # ... with 19 more variables: IncomePerCapErr &amp;lt;dbl&amp;gt;, Poverty &amp;lt;dbl&amp;gt;,
## #   ChildPoverty &amp;lt;dbl&amp;gt;, Professional &amp;lt;dbl&amp;gt;, Service &amp;lt;dbl&amp;gt;, Office &amp;lt;dbl&amp;gt;,
## #   Production &amp;lt;dbl&amp;gt;, Drive &amp;lt;dbl&amp;gt;, Carpool &amp;lt;dbl&amp;gt;, Transit &amp;lt;dbl&amp;gt;,
## #   OtherTransp &amp;lt;dbl&amp;gt;, WorkAtHome &amp;lt;dbl&amp;gt;, MeanCommute &amp;lt;dbl&amp;gt;, Employed &amp;lt;dbl&amp;gt;,
## #   PrivateWork &amp;lt;dbl&amp;gt;, SelfEmployed &amp;lt;dbl&amp;gt;, FamilyWork &amp;lt;dbl&amp;gt;,
## #   Unemployment &amp;lt;dbl&amp;gt;, Minority &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 27
##   State    Men Women White Citizen Income IncomeErr IncomePerCap IncomePerCapErr
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Calif~  50.4 2665.  55.7    68.2 59031.     9828.       28014.           3977.
## # ... with 18 more variables: Poverty &amp;lt;dbl&amp;gt;, ChildPoverty &amp;lt;dbl&amp;gt;,
## #   Professional &amp;lt;dbl&amp;gt;, Service &amp;lt;dbl&amp;gt;, Office &amp;lt;dbl&amp;gt;, Production &amp;lt;dbl&amp;gt;,
## #   Drive &amp;lt;dbl&amp;gt;, Carpool &amp;lt;dbl&amp;gt;, Transit &amp;lt;dbl&amp;gt;, OtherTransp &amp;lt;dbl&amp;gt;,
## #   WorkAtHome &amp;lt;dbl&amp;gt;, MeanCommute &amp;lt;dbl&amp;gt;, Employed &amp;lt;dbl&amp;gt;, PrivateWork &amp;lt;dbl&amp;gt;,
## #   SelfEmployed &amp;lt;dbl&amp;gt;, FamilyWork &amp;lt;dbl&amp;gt;, Unemployment &amp;lt;dbl&amp;gt;, Minority &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;exploratory-analysis&#34;&gt;Exploratory analysis&lt;/h1&gt;
&lt;ol start=&#34;15&#34;&gt;
&lt;li&gt;Carry out PCA for both county &amp;amp; sub-county level census data. Compute the first two principal components PC1 and PC2 for both county and sub-county respectively. Discuss whether you chose to center and scale the features and the reasons for your choice. Examine and interpret the loadings.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;County Level: 
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-27-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-27-3.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;**The first PC has a high percentage of children under poverty level, a high percentage of people under poverty level, and high percentage of those unemployed. The first PC also has low income per capital, low percentage of those employed (16+), and low median household income. I would label this principal component as &lt;em&gt;Under Poverty Level&lt;/em&gt;. **&lt;/p&gt;
&lt;p&gt;**The second PC has a high percentage of those who are self-employed, a high percentage of those working at home, and high percentage of those working in unpaid family work. The second PC also has low number of women, low percentage of working in private industry, and low percentage of employed in office jobs. I would label this principal component as &lt;em&gt;Working Class - Family Oriented&lt;/em&gt;. **&lt;/p&gt;
&lt;p&gt;Sub-county Level:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-28-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-28-3.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This PC is interesting because all 26 variables are positive and relatively high, where most have loadings greater than .20. The first PC has a high percentage of Men, a high number of citizens, high percentage of those employed in private industry, high percentage of those who commute alone in a car, and a high percentage of those employed (+16). I would label this principal component as &lt;em&gt;Working-Class&lt;/em&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The second PC has a high percentage of minorities, a high percentage of unemployment, and high percentage of child poverty and under poverty level. The second PC also has low percentage of those who work from home, low percentage of inpaid family work, and low percentage of those who are self-employed. I would label this principal component as &lt;em&gt;Under Poverty Level&lt;/em&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**It&amp;rsquo;s usually important to both center and scale your variables as they are most likely not on the same metric. As in this case, I center and scaled the dataset because some variables are count and others are percentages. Either way, it doesn&amp;rsquo;t hurt. **&lt;/p&gt;
&lt;ol start=&#34;16&#34;&gt;
&lt;li&gt;Determine the minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. Plot the proportion of variance explained and cumulative variance explained for both county and sub-county analyses.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;County Level:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;At least &lt;em&gt;13&lt;/em&gt; PCs are needed to capture 90% of the variance for the county analysis.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sub-County Level:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
**At least *7* PCs are needed to capture 90% of the variance for the sub-county analysis.**
&lt;ol start=&#34;17&#34;&gt;
&lt;li&gt;With &lt;code&gt;census_ct&lt;/code&gt;, perform hierarchical clustering with complete linkage.  Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components the county-level data as inputs instead of the original features. Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate cluster? Comment on what you observe and discuss possible explanations for these observations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hierarchical Clustering:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    clusters       n
##    &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt;
##  1 cluster 1   2307
##  2 cluster 2    779
##  3 cluster 3     67
##  4 cluster 4      8
##  5 cluster 5     20
##  6 cluster 6      4
##  7 cluster 7      3
##  8 cluster 8      3
##  9 cluster 9     23
## 10 cluster 10     4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cluster for San Mateo County:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] cluster 1
## 10 Levels: cluster 1 cluster 2 cluster 3 cluster 4 cluster 5 ... cluster 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hierarchical Clustering for first five PCs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    clusters       n
##    &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt;
##  1 cluster 1   1950
##  2 cluster 2    631
##  3 cluster 3    312
##  4 cluster 4     57
##  5 cluster 5      8
##  6 cluster 6    167
##  7 cluster 7     43
##  8 cluster 8      7
##  9 cluster 9     40
## 10 cluster 10     3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cluster for San Mateo County:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] cluster 3
## 10 Levels: cluster 1 cluster 2 cluster 3 cluster 4 cluster 5 ... cluster 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;It seems that the counties in the PC cluster are more evenly spread out than using the entire dataset. When using the first 5 principal components, we are capturing 64% of the variance in the county dataset with only five variables. Because of the smaller dimension, it may be more effective in clustering the counties as opposed to using 26 variables with the full dataset. It makes sense that San Mateo county is put into cluster 1 with the full dataset as most counties are grouped there. Thus, we may want to rely on the cluster that used the PCs, where San Mateo is in cluster 3.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;classification&#34;&gt;Classification&lt;/h1&gt;
&lt;p&gt;In order to train classification models, we need to combine &lt;code&gt;county_winner&lt;/code&gt; and &lt;code&gt;census_ct&lt;/code&gt; data. This seemingly straightforward task is harder than it sounds. Codes are provided in the .Rmd file that make the necessary changes to merge them into &lt;code&gt;election_cl&lt;/code&gt; for classification.&lt;/p&gt;
&lt;p&gt;After merging the data, partition the result into 80% training and 20% testing partitions.&lt;/p&gt;
&lt;ol start=&#34;18&#34;&gt;
&lt;li&gt;Decision tree: train a decision tree on the training partition, and apply cost-complexity pruning. Visualize the tree before and after pruning. Estimate the misclassification errors on the test partition, and interpret and discuss the results of the decision tree analysis. Use your plot to tell a story about voting behavior in the US (see this &lt;a href=&#34;https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NYT infographic&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## 
## Classification tree:
## tree(formula = as.factor(candidate) ~ ., data = train, control = tree_opts, 
##     split = &amp;quot;deviance&amp;quot;)
## Variables actually used in tree construction:
##  [1] &amp;quot;White&amp;quot;           &amp;quot;Men&amp;quot;             &amp;quot;Unemployment&amp;quot;    &amp;quot;IncomePerCapErr&amp;quot;
##  [5] &amp;quot;MeanCommute&amp;quot;     &amp;quot;Service&amp;quot;         &amp;quot;Women&amp;quot;           &amp;quot;Citizen&amp;quot;        
##  [9] &amp;quot;IncomePerCap&amp;quot;    &amp;quot;Drive&amp;quot;           &amp;quot;Transit&amp;quot;         &amp;quot;Office&amp;quot;         
## [13] &amp;quot;Production&amp;quot;      &amp;quot;SelfEmployed&amp;quot;    &amp;quot;Income&amp;quot;          &amp;quot;WorkAtHome&amp;quot;     
## [17] &amp;quot;Poverty&amp;quot;         &amp;quot;Employed&amp;quot;        &amp;quot;Carpool&amp;quot;         &amp;quot;Professional&amp;quot;   
## [21] &amp;quot;PrivateWork&amp;quot;     &amp;quot;OtherTransp&amp;quot;     &amp;quot;FamilyWork&amp;quot;     
## Number of terminal nodes:  84 
## Residual mean deviance:  0.04184 = 99.29 / 2373 
## Misclassification error rate: 0.008547 = 21 / 2457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tree before pruning:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;Add cost-complexity pruning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Classification tree:
## snip.tree(tree = t_small, nodes = c(4L, 14L, 5L, 15L, 6L))
## Variables actually used in tree construction:
## [1] &amp;quot;White&amp;quot;        &amp;quot;Men&amp;quot;          &amp;quot;Transit&amp;quot;      &amp;quot;Professional&amp;quot;
## Number of terminal nodes:  5 
## Residual mean deviance:  0.4521 = 1108 / 2452 
## Misclassification error rate: 0.0814 = 200 / 2457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tree after pruning:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;Classification Errors on Test Partition - Before Pruning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                  pred
## class             Donald Trump Hillary Clinton
##   Donald Trump       0.9496124       0.0503876
##   Hillary Clinton    0.3505155       0.6494845
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Classification Errors on Test Partition - After Pruning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                  pred
## class             Donald Trump Hillary Clinton
##   Donald Trump      0.94379845      0.05620155
##   Hillary Clinton   0.44329897      0.55670103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;After training a decision tree before pruning, the resulting classification tree had 84 terminal nodes and a misclassification error rate of 0.0085. There were 23 variables used. After pruning the classification tree, now there are 5 terminal nodes and a misclassifcation error rate of 0.0814. There were four variables used: White, Men, Transit, Professional. Similar to the infographic, it&amp;rsquo;s not surprising that &amp;ldquo;White&amp;rdquo; is used as the first splitting variable and Men is one of the second spliting variables. Counties that are less than 49.969% White and less than 49.6965 Male were likely to vote for Hillary Clinton. Which makes sense given the demographics of the election results, however it isn&amp;rsquo;t very telling as splitting on almost 50% isn&amp;rsquo;t much information. On the other side of the decision tree, counties that are more than almost 50% white, more than 1.138% use public transportation, and more than 35.4651% are employed in management positions voted for Hillary. It&amp;rsquo;s interesting that counties that have more than 1.138% of people using public transit are likely to vote Trump. It may indicate that the model needs some adjustments since the misclassification error rate is higher for this model.&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;19&#34;&gt;
&lt;li&gt;Train a logistic regression model on the training partition to predict the winning candidate in each county and estimate errors on the test partition. What are the significant variables? Are these consistent with what you observed in the decision tree analysis? Interpret the meaning of one or two significant coefficients of your choice in terms of a unit change in the variables. Did the results in your particular county (from question 14) match the predicted results?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = as.factor(candidate) ~ ., family = &amp;quot;binomial&amp;quot;, 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.9700  -0.2610  -0.1094  -0.0414   3.5310  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)     -1.230e+01  9.576e+00  -1.284 0.199042    
## Men             -3.202e-02  5.466e-02  -0.586 0.558020    
## Women            4.849e-05  1.675e-04   0.289 0.772235    
## White           -1.876e-01  6.417e-02  -2.924 0.003453 ** 
## Citizen          1.139e-01  2.934e-02   3.881 0.000104 ***
## Income          -5.835e-05  2.752e-05  -2.120 0.033970 *  
## IncomeErr        4.269e-05  6.807e-05   0.627 0.530554    
## IncomePerCap     1.900e-04  6.601e-05   2.879 0.003991 ** 
## IncomePerCapErr -2.421e-04  1.405e-04  -1.723 0.084840 .  
## Poverty          4.498e-02  4.267e-02   1.054 0.291820    
## ChildPoverty    -4.893e-03  2.611e-02  -0.187 0.851348    
## Professional     2.568e-01  3.852e-02   6.665 2.64e-11 ***
## Service          3.509e-01  4.974e-02   7.055 1.73e-12 ***
## Office           1.209e-01  4.660e-02   2.595 0.009448 ** 
## Production       1.849e-01  4.262e-02   4.337 1.44e-05 ***
## Drive           -2.236e-01  4.639e-02  -4.821 1.43e-06 ***
## Carpool         -2.164e-01  6.138e-02  -3.525 0.000423 ***
## Transit          4.788e-02  9.227e-02   0.519 0.603815    
## OtherTransp     -9.843e-02  1.018e-01  -0.967 0.333650    
## WorkAtHome      -1.356e-01  7.447e-02  -1.821 0.068677 .  
## MeanCommute      5.417e-02  2.500e-02   2.167 0.030224 *  
## Employed         1.960e-01  3.376e-02   5.807 6.36e-09 ***
## PrivateWork      7.211e-02  2.230e-02   3.233 0.001224 ** 
## SelfEmployed     1.535e-02  4.992e-02   0.308 0.758415    
## FamilyWork      -8.790e-01  4.130e-01  -2.128 0.033336 *  
## Unemployment     1.742e-01  4.075e-02   4.275 1.91e-05 ***
## Minority        -6.081e-02  6.186e-02  -0.983 0.325579    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2071.81  on 2456  degrees of freedom
## Residual deviance:  841.25  on 2430  degrees of freedom
## AIC: 895.25
## 
## Number of Fisher Scoring iterations: 7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Errors (Test Partition):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                  y_hat_glm
##                           No        Yes
##   Donald Trump    0.95930233 0.04069767
##   Hillary Clinton 0.32989691 0.67010309
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Significant predictors of candidate are White, Citizen, Income, IncomePerCap, Professional, Service, Office, Production, Drive, Carpool, MeanCommute, Employed, PrivateWork, FamilyWork, and Unemployement. Only one of the variables overlap with the variables chosen in the pruned decision tree which is inconsistent.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The probability of a county voting for Donald Trump increases by an estimated 0.1139 among counties with Citizens, after accounting for all other predictors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The probability of a county voting for Donald Trump decreases by an estimated -.879 among counties with unpaid family workers, after accounting for all other predictors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yes, the results in Los Angeles county (Hillary Clinton) matched the predicted results.&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;20&#34;&gt;
&lt;li&gt;Compute ROC curves for the decision tree and logistic regression using predictions on the test data, and display them on the same plot. Based on your classification results, discuss the pros and cons of each method. Are the different classifiers more appropriate for answering different kinds of questions about the election?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Decision Tree:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;Logistic Regression:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Both ROC curves look very different, with the logistic curve looking cleaner and has a higher optimal threshold. Both analyses have pros and cons and different classifiers are more appropriate for answering different kinds of questions about the election. Regarding decision trees, as we saw in the output, they are intuitive and easy to implement visualize and interpret. They also have the ability to be pruned by cost complexity. However, they are sensitive to small changes in the data, have high variance, and could fail if they separation in the feature space is hard to approximate. I think working with this scale of data is difficult to implement decision trees with because there is a lot of variability in data. Regarding logistic regression, they are interpretable, flexible, and can take in a large number of predictors. However, like decision trees, they can fail if data are separated in the feature space.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;taking-it-further&#34;&gt;Taking it further&lt;/h1&gt;
&lt;ol start=&#34;21&#34;&gt;
&lt;li&gt;This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations. Use any tools at your disposal to make your case: visualize errors on the map, discuss what does or doesn&amp;rsquo;t seem reasonable based on your understanding of these methods, propose possible directions (for example, collecting additional data or domain knowledge).  In addition, propose and tackle &lt;em&gt;at least&lt;/em&gt; one more interesting question. Creative and thoughtful analyses will be rewarded!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;As mentioned previously, both analyses have pros and cons and different classifiers are more appropriate for answering different kinds of questions about the election. In this example, it is shown that the logistic model performed better, providing a better misclassification error rate. I think the model would perform even better with theory-based predictors as that can influence the model, more predictors doesn&amp;rsquo;t necessarily mean better.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of using the native attributes (the original features), we can use principal components to create new (and lower dimensional) sets of features with which to train a classification model. This sometimes improves classification performance.  Compare classifiers trained on the original features with those trained on PCA features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;I like the idea of using PC components to reevaluate the classification methods used. The minimum number of PCs needed to capture 90% of the variance for county data is &lt;em&gt;13&lt;/em&gt;, however, using 13 components makes interpretation tricky, as evaluating and labeling all 13 will be extremely time-consulting. To assess differences in bias-variance trade off between using PC and raw data, I will rerun both the decision tree analysis and the logistic regress using the first 4 principal components to predict the election results and compare.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rerun PCs with combined &lt;code&gt;county_winner&lt;/code&gt; column for easier wrangling:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-47-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note: Eigenvalues are slightly off because some observations were removed in the process of combining &lt;code&gt;county_winner&lt;/code&gt; and &lt;code&gt;census_ct&lt;/code&gt; but they are generally the same.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PC 1/Affluent&lt;/em&gt;: High IncomePerCap, Employed, Income, and Professional. Low Child Poverty, Poverty, and Unemployment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PC 2/Family Oriented&lt;/em&gt;: High SelfEmployed, WorkAtHome, and FamilyWork. Low Women, PrivateWork, IncomeErr.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PC 3/Working Class in Suburb&lt;/em&gt;: High White, Drive, Production, and Private Work. Low Minority, and Transit.&lt;/p&gt;
&lt;p&gt;*PC 4/Working Class in City *: High Carpool, Production, Employed. Low Citizen, Service, Office.&lt;/p&gt;
&lt;p&gt;Decision Tree:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Classification tree:
## tree(formula = as.factor(candidate) ~ ., data = train, control = tree_opts, 
##     split = &amp;quot;deviance&amp;quot;)
## Number of terminal nodes:  56 
## Residual mean deviance:  0.3242 = 778.3 / 2401 
## Misclassification error rate: 0.0639 = 157 / 2457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tree before pruning:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-50-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;Add cost-complexity pruning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Classification tree:
## snip.tree(tree = t_small, nodes = c(10L, 7L, 54L, 55L, 26L, 91L, 
## 23L, 25L, 90L, 9L))
## Number of terminal nodes:  13 
## Residual mean deviance:  0.4694 = 1147 / 2444 
## Misclassification error rate: 0.09565 = 235 / 2457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tree after pruning:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-52-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;Classification Errors on Test Partition - Before Pruning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                  pred
## class             Donald Trump Hillary Clinton
##   Donald Trump      0.95348837      0.04651163
##   Hillary Clinton   0.49484536      0.50515464
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Classification Errors on Test Partition - After Pruning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                  pred
## class             Donald Trump Hillary Clinton
##   Donald Trump      0.93410853      0.06589147
##   Hillary Clinton   0.51546392      0.48453608
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;After training a decision tree before pruning, the resulting classification tree had 56 terminal nodes and a misclassification error rate of 0.0639. The four PCs were used. Already, we see that the misclassifciation error is much higher than using the full dataset shown earlier. After pruning the classification tree, now there are 13 terminal nodes and a misclassifcation error rate of 0.09565. Compared to the full dataset the pruned decision tree had less terminal nodes (5), but the misclassification error rate was slightly similar (0.08). The interpretation is slightly different now that we have to identify the PC on the tree. For example, if the loading on PC3 is less than -.610 (or in this case less affluent counties) and the loading on PC2 is less than -1.47912 (or in this case less family oriented counties), and the loading on PC3 is less than -3.40997 (even less affleunt counties) then it is likely they voted for Hillary Clinton. Already, this interpretation is extremely confusing compared to using all predictors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Logistic Regression:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = as.factor(candidate) ~ ., family = &amp;quot;binomial&amp;quot;, 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.1258  -0.4289  -0.2081  -0.1065   3.3147  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -2.76655    0.11026 -25.091  &amp;lt; 2e-16 ***
## PC1         -0.05276    0.02655  -1.987   0.0469 *  
## PC2         -0.64696    0.04784 -13.523  &amp;lt; 2e-16 ***
## PC3         -0.86938    0.05006 -17.368  &amp;lt; 2e-16 ***
## PC4         -0.39303    0.06011  -6.538 6.23e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2071.8  on 2456  degrees of freedom
## Residual deviance: 1262.3  on 2452  degrees of freedom
## AIC: 1272.3
## 
## Number of Fisher Scoring iterations: 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;All four PCs are significant predictors of candidate. And it looks like all for PCs have similar interpretation: the probability of a county voting for Donald Trump decreases for each PC estimate. This was not the case when all variables were entered in the logistic model.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Errors (Test Partition):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                  y_hat_glm
##                           No        Yes
##   Donald Trump    0.95155039 0.04844961
##   Hillary Clinton 0.52577320 0.47422680
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The true positve for Hillary Clinton in the model is a lot less than the model with all variables, however.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ROC Curves:&lt;/p&gt;
&lt;p&gt;Decision Tree:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;Logistic Regression:&lt;/p&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/machine-learning-final-project/index_files/figure-html/unnamed-chunk-58-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Overall, it seems like using the first four principal components did not improve neither the decision tree or logistic regression model. This could be due to many factors but the main one I can think of is that the first four PCs only capture 59% of the variance in the variables. The decision tree did not make sense to use even if it had a better misclassification rate as the interpretability of the tree did not as much sense as the model with all variables. While the logistic model did have all significant predictors, the error rate was much higher than the model with all variables, but less predictors were used. In the future, I would select variables based on theory, use PCA to reduce the dimensions and capture more variance than I did now, and then re-run the model.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nonlinear Least Squares Example</title>
      <link>https://dinaarch.netlify.app/post/nonlinear-least-squares-example/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/post/nonlinear-least-squares-example/</guid>
      <description>&lt;h2 id=&#34;parameter-estimation---wild-fish-catch&#34;&gt;Parameter Estimation - Wild Fish Catch&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;: Source: Global wild fish catch and aquaculture production, compiled by Earth Policy Institute within 1950-2010 from U.N. Food and Agriculture Organization (FAO), Global Capture Production and Global Aquaculture Production, electronic databases, at &lt;a href=&#34;http://www.fao.org/fishery/topic/16140/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.fao.org/fishery/topic/16140/en&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;“For Task 2, you will find an equation with parameters estimated by nonlinear least squares for the increase in global wild fish catch from 1950 – 2012.”&lt;/p&gt;
&lt;h3 id=&#34;data-wrangling&#34;&gt;Data Wrangling&lt;/h3&gt;
&lt;p&gt;Read in data and recode year for analysis set up:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fish_df &amp;lt;- read_csv(&amp;quot;fish_catch.csv&amp;quot;) %&amp;gt;% 
  mutate(year_coded = 0:62) #recoded year for analyses
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;a--b-graph-of-wild-catch-over-time&#34;&gt;A &amp;amp; B. Graph of wild catch over time&lt;/h3&gt;
&lt;p&gt;Create an exploratory graph over wild catch over time:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fish_df %&amp;gt;% 
  ggplot(aes(x = year, y = wild_catch_mil_tons)) +
  geom_point() +
  theme_minimal() +
  labs(x = &amp;quot;Years&amp;quot;, y = &amp;quot;Wild Catch (Million Tons)&amp;quot;, title = &amp;quot;Wild Catch Over Time (Years)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/nonlinear-least-squares-example/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Log transformed
fish_df %&amp;gt;% 
  ggplot(aes(x = year, y = log(wild_catch_mil_tons))) +
  geom_point() +
  theme_minimal() +
  labs(x = &amp;quot;Years&amp;quot;, y = &amp;quot;ln(Wild Catch)&amp;quot;, title = &amp;quot;Wild Catch Over Time (Years)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/nonlinear-least-squares-example/index_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;In this plot of wild catch over time, a possible logistic growth relatiosnhip describes the trend:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$P(t) = \frac{K} {1 + Ae^{-kt}}$$&lt;/code&gt;
where&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$A = \frac{K - P_0} {P_0}$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;My initial estimates for the parameters in the model are: &lt;code&gt;\(K\)&lt;/code&gt; = 90, &lt;code&gt;\(P_0\)&lt;/code&gt; = 20, &lt;code&gt;\(k\)&lt;/code&gt; = .05, &lt;code&gt;\(A\)&lt;/code&gt; = 3.5.&lt;/p&gt;
&lt;h3 id=&#34;c-nonlinear-least-squares&#34;&gt;C. Nonlinear least squares&lt;/h3&gt;
&lt;p&gt;Using nonlinear least squares (NLS) to find parameters for the wild catch model. First, we estimate &lt;code&gt;\(k\)&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_exp &amp;lt;- fish_df %&amp;gt;% 
  filter(year &amp;lt; 1990) %&amp;gt;% 
  mutate(ln_fish = log(wild_catch_mil_tons))
  
lm_k &amp;lt;- lm(ln_fish ~ year, data = df_exp)
lm_k
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = ln_fish ~ year, data = df_exp)
## 
## Coefficients:
## (Intercept)         year  
##   -66.38553      0.03562
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Coefficient (k) ~ 0.04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The initial coefficient &lt;code&gt;\(k\)&lt;/code&gt; = 0.4.&lt;/p&gt;
&lt;p&gt;Then, we use &lt;code&gt;stats::nls()&lt;/code&gt; with the estimated starting parameter values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_nls &amp;lt;- nls(wild_catch_mil_tons ~ K/(1 + A*exp(-r*year_coded)),
              data = fish_df,
              start = list(K = 90, A = 3.5, r = 0.4),
              trace = FALSE
              )

# See the model summary
#summary(df_nls)

# Use broom:: functions to get model outputs in tidier format: 
model_out &amp;lt;- broom::tidy(df_nls) 

model_out %&amp;gt;% 
  gt()
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;ppreqatnez&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}
&lt;p&gt;#ppreqatnez .gt_table {
display: table;
border-collapse: collapse;
margin-left: auto;
margin-right: auto;
color: #333333;
font-size: 16px;
font-weight: normal;
font-style: normal;
background-color: #FFFFFF;
width: auto;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #A8A8A8;
border-right-style: none;
border-right-width: 2px;
border-right-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #A8A8A8;
border-left-style: none;
border-left-width: 2px;
border-left-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_heading {
background-color: #FFFFFF;
text-align: center;
border-bottom-color: #FFFFFF;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_title {
color: #333333;
font-size: 125%;
font-weight: initial;
padding-top: 4px;
padding-bottom: 4px;
border-bottom-color: #FFFFFF;
border-bottom-width: 0;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_subtitle {
color: #333333;
font-size: 85%;
font-weight: initial;
padding-top: 0;
padding-bottom: 6px;
border-top-color: #FFFFFF;
border-top-width: 0;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_bottom_border {
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_col_headings {
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_col_heading {
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: normal;
text-transform: inherit;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
vertical-align: bottom;
padding-top: 5px;
padding-bottom: 6px;
padding-left: 5px;
padding-right: 5px;
overflow-x: hidden;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_column_spanner_outer {
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: normal;
text-transform: inherit;
padding-top: 0;
padding-bottom: 0;
padding-left: 4px;
padding-right: 4px;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_column_spanner_outer:first-child {
padding-left: 0;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_column_spanner_outer:last-child {
padding-right: 0;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_column_spanner {
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
vertical-align: bottom;
padding-top: 5px;
padding-bottom: 5px;
overflow-x: hidden;
display: inline-block;
width: 100%;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_group_heading {
padding: 8px;
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: initial;
text-transform: inherit;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
vertical-align: middle;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_empty_group_heading {
padding: 0.5px;
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: initial;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
vertical-align: middle;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_from_md &amp;gt; :first-child {
margin-top: 0;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_from_md &amp;gt; :last-child {
margin-bottom: 0;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_row {
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
margin: 10px;
border-top-style: solid;
border-top-width: 1px;
border-top-color: #D3D3D3;
border-left-style: none;
border-left-width: 1px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 1px;
border-right-color: #D3D3D3;
vertical-align: middle;
overflow-x: hidden;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_stub {
color: #333333;
background-color: #FFFFFF;
font-size: 100%;
font-weight: initial;
text-transform: inherit;
border-right-style: solid;
border-right-width: 2px;
border-right-color: #D3D3D3;
padding-left: 12px;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_summary_row {
color: #333333;
background-color: #FFFFFF;
text-transform: inherit;
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_first_summary_row {
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_grand_summary_row {
color: #333333;
background-color: #FFFFFF;
text-transform: inherit;
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_first_grand_summary_row {
padding-top: 8px;
padding-bottom: 8px;
padding-left: 5px;
padding-right: 5px;
border-top-style: double;
border-top-width: 6px;
border-top-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_striped {
background-color: rgba(128, 128, 128, 0.05);
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_table_body {
border-top-style: solid;
border-top-width: 2px;
border-top-color: #D3D3D3;
border-bottom-style: solid;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_footnotes {
color: #333333;
background-color: #FFFFFF;
border-bottom-style: none;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 2px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 2px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_footnote {
margin: 0px;
font-size: 90%;
padding: 4px;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_sourcenotes {
color: #333333;
background-color: #FFFFFF;
border-bottom-style: none;
border-bottom-width: 2px;
border-bottom-color: #D3D3D3;
border-left-style: none;
border-left-width: 2px;
border-left-color: #D3D3D3;
border-right-style: none;
border-right-width: 2px;
border-right-color: #D3D3D3;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_sourcenote {
font-size: 90%;
padding: 4px;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_left {
text-align: left;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_center {
text-align: center;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_right {
text-align: right;
font-variant-numeric: tabular-nums;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_font_normal {
font-weight: normal;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_font_bold {
font-weight: bold;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_font_italic {
font-style: italic;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_super {
font-size: 65%;
}&lt;/p&gt;
&lt;p&gt;#ppreqatnez .gt_footnote_marks {
font-style: italic;
font-weight: normal;
font-size: 65%;
}
&lt;/style&gt;&lt;/p&gt;
&lt;table class=&#34;gt_table&#34;&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;term&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;estimate&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;std.error&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;statistic&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;p.value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;K&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;100.27835748&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;2.734132433&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;36.67648&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;8.569260e-43&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;A&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;4.31633154&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;0.292941193&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;14.73446&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;1.340788e-21&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;r&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;0.06988688&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;0.004648183&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;15.03531&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;5.141952e-22&lt;/td&gt;&lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The model with estimated parameters for wild catch over time is:
&lt;code&gt;$$P(t) = \frac{100.3}{1+4.32e^{-0.07t}}$$&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;d-visualize-model-showing-both-original-data-and-model-output&#34;&gt;D) Visualize model showing both original data and model output&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make predictions for the wild catch at years: 
p_predict &amp;lt;- predict(df_nls)

# Bind predictions to original data frame:
df_complete &amp;lt;- data.frame(fish_df, p_predict)

# Plot them all together:
ggplot(data = df_complete, aes(x = year, y = wild_catch_mil_tons)) +
  geom_point() +
  geom_line(aes(x = year, y = p_predict)) +
  theme_calc() +
  scale_x_continuous(breaks = seq(1950, 2012, 10)) +
  labs(title = &amp;quot;Wild Catch (Million Tons) Over Time&amp;quot;, x = &amp;quot;Year&amp;quot;, y = &amp;quot;Wild Catch (Million Tons)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/nonlinear-least-squares-example/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>Text Anaylsis ft. Michael Scott</title>
      <link>https://dinaarch.netlify.app/post/text-analysis/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/post/text-analysis/</guid>
      <description>&lt;h2 id=&#34;the-office---season-4-x-episode-9-the-dinner-party&#34;&gt;The Office - Season 4 x Episode 9: The Dinner Party&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Dinner Party&#34; srcset=&#34;
               /post/text-analysis/featured_hu3f0f31071d748846f7fc056e73b9dc63_124538_bf8720d9fa16957f5c280ddce1fb3b1e.jpg 400w,
               /post/text-analysis/featured_hu3f0f31071d748846f7fc056e73b9dc63_124538_1ebeb7d0e9d212a394bb27cf7d70d48b.jpg 760w,
               /post/text-analysis/featured_hu3f0f31071d748846f7fc056e73b9dc63_124538_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://dinaarch.netlify.app/post/text-analysis/featured_hu3f0f31071d748846f7fc056e73b9dc63_124538_bf8720d9fa16957f5c280ddce1fb3b1e.jpg&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This exercise is atext and sentiment analysis on the &amp;ldquo;Dinner Party&amp;rdquo; episode of The Office. The assignment was given in Dr. Allison Horst&amp;rsquo;s ESM 244 course.&lt;/p&gt;
&lt;h2 id=&#34;read-in-the-text-transcript&#34;&gt;Read in the text transcript&lt;/h2&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://www.officequotes.net/no4-09.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.officequotes.net/no4-09.php&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_text &amp;lt;- read_delim(here(&amp;quot;content&amp;quot;,&amp;quot;post&amp;quot;, &amp;quot;2021-12-14-text-analysis&amp;quot;, &amp;quot;dinnerparty2.txt&amp;quot;), 
                          &amp;quot;\t&amp;quot;, escape_double = FALSE, col_names = FALSE, 
                          trim_ws = TRUE) %&amp;gt;% 
  rename(lines = X1)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Each row is a a line of the transcript for each character.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example: First line from the episode is from Stanley Hudson:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_line1 &amp;lt;- dinner_text[1,]

dinner_line1 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   lines                       
##   &amp;lt;chr&amp;gt;                       
## 1 Stanley: This is ridiculous.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tidy-data&#34;&gt;Tidy Data&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;I separated the character names and their lines into two columns, &lt;code&gt;character&lt;/code&gt; and &lt;code&gt;line&lt;/code&gt;, separated by colon.&lt;/li&gt;
&lt;li&gt;Removed the last four rows that are not character&amp;rsquo;s lines but stage direction.&lt;/li&gt;
&lt;li&gt;I  filtered the characters to include the characters that spoke the most words.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_tidy &amp;lt;- dinner_text %&amp;gt;% 
  separate(col = lines, into = c(&amp;quot;character&amp;quot;, &amp;quot;line&amp;quot;), sep = &amp;quot;:&amp;quot;) %&amp;gt;% 
  slice(-(283:288)) %&amp;gt;% 
  filter(character %in% c(&amp;quot;Andy&amp;quot;, &amp;quot;Angela&amp;quot;, &amp;quot;Dwight&amp;quot;, &amp;quot;Michael&amp;quot;, &amp;quot;Hunter&#39;s CD&amp;quot;, &amp;quot;Jan&amp;quot;, &amp;quot;Jim&amp;quot;, &amp;quot;Pam&amp;quot;)) 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tokenize&#34;&gt;Tokenize&lt;/h2&gt;
&lt;p&gt;Here, we get word counts for each character for the episode.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_tokens &amp;lt;- dinner_tidy %&amp;gt;% 
  unnest_tokens(word, line) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_wordcount &amp;lt;- dinner_tokens %&amp;gt;% 
  count(character, word)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;remove-stop-words&#34;&gt;Remove stop words&lt;/h2&gt;
&lt;p&gt;Most of the words in the transcript are stop words. To remove them, we use &lt;code&gt;anti_join&lt;/code&gt; and the &lt;code&gt;stop_words&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_nonstop_words &amp;lt;- dinner_tokens %&amp;gt;% 
  anti_join(stop_words)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then recount with the stopwords removed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nonstop_counts &amp;lt;- dinner_nonstop_words %&amp;gt;% 
  count(character, word) 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;top-10-words&#34;&gt;Top 10 Words&lt;/h2&gt;
&lt;p&gt;Here, we find the top 10 words that each character said during that episode.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;top_10_words &amp;lt;- nonstop_counts %&amp;gt;% 
  group_by(character) %&amp;gt;% 
  arrange(-n) %&amp;gt;% 
  slice(1:10)


top_10_words %&amp;gt;% 
  group_by(word) %&amp;gt;% 
  ggplot() +
  geom_bar(aes(reorder(word, n), n), stat = &#39;identity&#39;, fill = &amp;quot;red&amp;quot;) +
  facet_wrap(~character, scales = &amp;quot;free&amp;quot;) +
  coord_flip() +
  theme_calc() +
  labs(x = &#39;word&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/text-analysis/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;word-cloud&#34;&gt;Word Cloud&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make a word cloud for the top 5 characters who spoke the most:&lt;/p&gt;
&lt;p&gt;Michael, Jan, Jim, Pam, Dwight&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nonstop_counts %&amp;gt;% 
  group_by(character) %&amp;gt;% 
  count() %&amp;gt;% 
  arrange(desc(n)) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
## # Groups:   character [8]
##   character       n
##   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
## 1 Michael       275
## 2 Jan           190
## 3 Jim            88
## 4 Pam            72
## 5 Dwight         38
## 6 Angela         21
## 7 Andy           20
## 8 Hunter&#39;s CD     4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_top5 &amp;lt;- nonstop_counts %&amp;gt;% 
  filter(character %in% c(&amp;quot;Michael&amp;quot;, &amp;quot;Jan&amp;quot;, &amp;quot;Jim&amp;quot;, &amp;quot;Pam&amp;quot;, &amp;quot;Dwight&amp;quot;)) %&amp;gt;% 
  group_by(character) %&amp;gt;% 
  arrange(-n) %&amp;gt;% 
  slice(1:20)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cloud &amp;lt;- ggplot(data = dinner_top5, aes(label = word)) +
  geom_text_wordcloud(aes(color = n, size = n), shape = &amp;quot;diamond&amp;quot;) +
  scale_size_area(max_size = 6) +
  scale_color_gradientn(colors = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;darkgreen&amp;quot;)) +
  facet_wrap(~character) +
  theme_calc()

cloud
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/text-analysis/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#afinn lexicon
get_sentiments(lexicon = &amp;quot;afinn&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;sentiment-analysis-with-afinn&#34;&gt;Sentiment analysis with afinn:&lt;/h3&gt;
&lt;p&gt;First, bind words in &lt;code&gt;dinner_nonstop_words&lt;/code&gt; to &lt;code&gt;afinn&lt;/code&gt; lexicon:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dinner_afinn &amp;lt;- dinner_nonstop_words %&amp;gt;% 
  inner_join(get_sentiments(&amp;quot;afinn&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we find counts based on &lt;code&gt;afinn&lt;/code&gt; lexicon and plot them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;afinn_counts &amp;lt;- dinner_afinn %&amp;gt;% 
  count(character, value)

# Plot them: 
ggplot(data = afinn_counts, aes(x = value, y = n)) +
  geom_col(fill = &amp;quot;blue&amp;quot;) +
  facet_wrap(~character) +
  theme_calc() +
  labs(y = &amp;quot;&amp;quot;, x = &amp;quot;Lexicon Value&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/text-analysis/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Find the mean afinn score by characeter: 
afinn_means &amp;lt;- dinner_afinn %&amp;gt;% 
  group_by(character) %&amp;gt;% 
  summarize(mean_afinn = mean(value))

ggplot(data = afinn_means, 
       aes(x = fct_rev(as.factor(character)), 
           y = mean_afinn)) +
  geom_col(fill = &amp;quot;blue&amp;quot;) +
  coord_flip() +
  theme_calc() +
  labs(x = &amp;quot;Character&amp;quot;, y = &amp;quot;Mean&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://dinaarch.netlify.app/post/text-analysis/index_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://dinaarch.netlify.app/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://dinaarch.netlify.app/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://dinaarch.netlify.app/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://dinaarch.netlify.app/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://dinaarch.netlify.app/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://dinaarch.netlify.app/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://dinaarch.netlify.app/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dinaarch.netlify.app/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
